{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T14:42:25.481837Z",
     "start_time": "2025-02-17T14:41:59.488811Z"
    }
   },
   "source": [
    "import difflib\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from language_classifier.language_classifier import LanguageClassifier\n",
    "\n",
    "# formatting\n",
    "pd.set_option('display.float_format', '{:.1f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# import data",
   "id": "a9aae87a2e850e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T14:42:25.551878Z",
     "start_time": "2025-02-17T14:42:25.489356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# folders\n",
    "parsed_docs_folder = os.path.join(\"..\", \"ParsedPublications\")\n",
    "fr_eng_correlation_csv = \"fr_eng_correlation_data.csv\"\n",
    "\n",
    "fr_eng_correlation_df = pd.read_csv(fr_eng_correlation_csv)\n",
    "\n",
    "# weblinks for previewing / testing\n",
    "weblinks_df = fr_eng_correlation_df.copy()\n",
    "weblinks_df = weblinks_df[['pub_number', 'nom', 'name', 'url_fr', 'url_en', 'file_url_fr', 'file_url_en']]\n",
    "\n",
    "# simplified correlation table\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[['pub_number', 'filename_fr', 'filename_en']]"
   ],
   "id": "83675b54298a8486",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# helper functions",
   "id": "ca8dc9ee3a7f7d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T19:31:25.000254Z",
     "start_time": "2025-02-17T19:31:24.973101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATA CLEANING FUNCTIONS\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    allowed_chars = r\"[^a-zA-ZÀ-ÖØ-öø-ÿ.,;:!?()'\\\"-]\"\n",
    "    text = re.sub(allowed_chars, ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_files_for_publication(pub_number, fr_eng_correlation_df):\n",
    "    row = fr_eng_correlation_df.loc[fr_eng_correlation_df['pub_number'] == pub_number]\n",
    "    if not row.empty:\n",
    "        filename_fr = row['filename_fr'].values[0]\n",
    "        filename_en = row['filename_en'].values[0]\n",
    "        return filename_fr, filename_en\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_json_file_link(parsed_docs_folder, pdf_filename):\n",
    "    if pdf_filename.endswith(\".pdf\"):\n",
    "        json_filename = pdf_filename + \".json\"\n",
    "        for root, _, files in os.walk(parsed_docs_folder):\n",
    "            if json_filename in files:\n",
    "                return os.path.join(root, json_filename)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text_from_single_file(json_file, target_language, clf):\n",
    "    min_block_length = 10\n",
    "    max_block_length = 500\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if 'text' not in data:\n",
    "        raise KeyError(f\"The key 'text' is missing in the JSON file: {json_file}\")\n",
    "    \n",
    "    full_text = clean_text(data['text'])\n",
    "    text_blocks = re.split(r'(?<![;,])[.?!]\\s|\\n\\n', full_text)\n",
    "    text = []\n",
    "\n",
    "    for block in text_blocks:\n",
    "        block = block.strip()\n",
    "        if len(block) < min_block_length or len(block) > max_block_length:\n",
    "            continue\n",
    "        \n",
    "        if clf.classify(block) == target_language:\n",
    "            text.append(block + '. ')      \n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def extract_both_languages_from_two_files(json_file_fr, json_file_en, clf):\n",
    "    return extract_text_from_single_file(json_file_fr, \"fr\", clf), extract_text_from_single_file(json_file_en, \"en\", clf)\n",
    "\n",
    "\n",
    "def extract_both_languages_from_single_file(json_file, clf):\n",
    "    min_block_length = 10\n",
    "    max_block_length = 500\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if 'text' not in data:\n",
    "        raise KeyError(f\"The key 'text' is missing in the JSON file: {json_file}\")\n",
    "    \n",
    "    full_text = data['text']\n",
    "    text_blocks = re.split(r'(?<![;,])[.?!]\\s|\\n\\n', full_text)\n",
    "    text_fr, text_en = [], []\n",
    "\n",
    "    for block in text_blocks:\n",
    "        block = block.strip()\n",
    "        if len(block) < min_block_length or len(block) > max_block_length:\n",
    "            continue\n",
    "            \n",
    "        if clf.classify(block) == \"fr\":\n",
    "            text_fr.append(block + '. ')   \n",
    "        elif clf.classify(block) == \"en\":\n",
    "            text_en.append(block + '. ')   \n",
    "\n",
    "    return \" \".join(text_fr), \" \".join(text_en)\n",
    "\n",
    "\n",
    "def create_sentences(text_fr, text_en):\n",
    "    sentences_fr = [x.strip() for x in re.split(r'(?<![;,])[.?!]\\s|\\n\\n', text_fr) if x != \"\"]\n",
    "    sentences_en = [x.strip() for x in re.split(r'(?<![;,])[.?!]\\s|\\n\\n', text_en) if x != \"\"]\n",
    "    \n",
    "    return sentences_fr, sentences_en\n",
    "\n",
    "\n",
    "def create_similarity_matrix(sentences_fr, sentences_en, sentence_encoder):\n",
    "    embeddings_fr = sentence_encoder.encode(sentences_fr, convert_to_tensor=True)\n",
    "    embeddings_en = sentence_encoder.encode(sentences_en, convert_to_tensor=True)\n",
    "\n",
    "    return util.pytorch_cos_sim(embeddings_fr, embeddings_en)\n",
    "\n",
    "\n",
    "def align_sentences(sim_matrix, threshold=0.7):\n",
    "    n, m = sim_matrix.shape\n",
    "    \n",
    "    weights = np.where(sim_matrix >= threshold, sim_matrix, 0.0)\n",
    "    \n",
    "    dp = np.zeros((n+1, m+1), dtype=np.float32)\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            score_match = dp[i-1, j-1] + weights[i-1, j-1]\n",
    "            score_skip_fr = dp[i-1, j]\n",
    "            score_skip_en = dp[i, j-1]\n",
    "            \n",
    "            dp[i, j] = max(score_match, score_skip_fr, score_skip_en)\n",
    "    \n",
    "    aligned_pairs = []\n",
    "    i, j = n, m\n",
    "    while i > 0 and j > 0:\n",
    "        current_val = dp[i, j]\n",
    "        if np.isclose(current_val, dp[i-1, j]):\n",
    "            i -= 1\n",
    "        elif np.isclose(current_val, dp[i, j-1]):\n",
    "            j -= 1\n",
    "        else:\n",
    "            if weights[i-1, j-1] > 0:\n",
    "                aligned_pairs.append((i-1, j-1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    \n",
    "    aligned_pairs.reverse()\n",
    "    \n",
    "    return aligned_pairs\n",
    "\n",
    "\n",
    "def text_from_coordinates(aligned_pairs, sentences_fr, sentences_en, pub_number):\n",
    "    correlated_list = list()\n",
    "    for i, j in aligned_pairs:\n",
    "        correlated_list.append((pub_number, sentences_fr[i], sentences_en[j]))\n",
    "    \n",
    "    return correlated_list\n",
    "\n",
    "\n",
    "def correlate_and_clean_text(text_fr, text_en, pub_number, sentence_encoder):\n",
    "    sentences_fr, sentences_en = create_sentences(text_fr, text_en)\n",
    "    similarity_matrix = create_similarity_matrix(sentences_fr, sentences_en, sentence_encoder)\n",
    "    aligned_pairs = align_sentences(similarity_matrix)\n",
    "\n",
    "    return text_from_coordinates(aligned_pairs, sentences_fr, sentences_en, pub_number)\n",
    "\n",
    "\n",
    "def process_all_rows(fr_eng_correlation_df, parsed_docs_folder, clf, sentence_encoder):\n",
    "    matched_data = []\n",
    "    max_ratio = 2  # low quality / only abstract data to exclude (<7% of total translated data)\n",
    "    min_char = 1000  # low quality, bad OCR, or incomplete transcription / parsing\n",
    "    \n",
    "    with alive_bar(fr_eng_correlation_df.shape[0], force_tty=True) as bar:\n",
    "        for _, row in fr_eng_correlation_df.iterrows():\n",
    "            bar()\n",
    "            \n",
    "            pub_number = row['pub_number']\n",
    "            filename_fr, filename_en = row['filename_fr'], row['filename_en']\n",
    "            \n",
    "            if filename_fr == \"WITHDRAWN\" and filename_en == \"WITHDRAWN\":\n",
    "                continue\n",
    "            \n",
    "            fr_link = get_json_file_link(parsed_docs_folder, filename_fr)\n",
    "            if fr_link == None:\n",
    "                continue\n",
    "            \n",
    "            if filename_fr == filename_en:\n",
    "                text_fr, text_en = extract_both_languages_from_single_file(fr_link, clf)\n",
    "            else:\n",
    "                en_link = get_json_file_link(parsed_docs_folder, filename_en) \n",
    "                if en_link == None:\n",
    "                    continue\n",
    "                text_fr, text_en = extract_both_languages_from_two_files(fr_link, en_link, clf)\n",
    "            \n",
    "            # low-quality text criteria\n",
    "            len_fr, len_en = len(text_fr), len(text_en)\n",
    "            if len_fr == 0 or len_en == 0:\n",
    "                continue\n",
    "            elif 1 / max_ratio > len(text_fr) / len(text_en) > max_ratio:\n",
    "                continue\n",
    "            elif len(text_fr) < min_char or len(text_en) < min_char:\n",
    "                continue\n",
    "            \n",
    "            list_of_correlated_text = correlate_and_clean_text(text_fr, text_en, pub_number, sentence_encoder)\n",
    "            matched_data.extend(list_of_correlated_text)\n",
    "        \n",
    "    return pd.DataFrame(matched_data, columns=['pub_number', 'fr', 'en'])\n"
   ],
   "id": "2677bfe14b892d1e",
   "outputs": [],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:38:28.128853Z",
     "start_time": "2025-02-17T19:31:26.104816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "language_classifier = LanguageClassifier()\n",
    "\n",
    "matched_df = process_all_rows(fr_eng_correlation_df, parsed_docs_folder, language_classifier, sentence_encoder)"
   ],
   "id": "89a46335416a3704",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 9061/9061 [100%] in 7:07:00.5 (0.35/s\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:38:28.947923Z",
     "start_time": "2025-02-18T02:38:28.214941Z"
    }
   },
   "cell_type": "code",
   "source": "matched_df.to_pickle(\"matched_data.pickle\")",
   "id": "bf44b1670443bcdc",
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8338a68fe712547f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e3d0db2f20471e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce25c888279980c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "838205f4e55ecb11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4aff3b959c5ae708"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
