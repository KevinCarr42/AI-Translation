{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T12:31:27.889564Z",
     "start_time": "2025-01-24T12:31:26.900928Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # jupyter notebook full-width display\n",
    "display(HTML(\"<style>.dataframe td { white-space: nowrap; }</style>\")) # no text wrapping\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.1f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.dataframe td { white-space: nowrap; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# import website url data",
   "id": "a9aae87a2e850e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T13:42:58.656429Z",
     "start_time": "2025-01-24T13:42:55.756279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "links_folder = \"website_reports\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in os.listdir(links_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(links_folder, file)\n",
    "        df = pd.read_excel(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "column_names = ['type', 'year', 'pub_number', '_', 'nom', 'name', 'url_fr', 'url_en', '_', '_', '_', '_']\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df.columns = column_names\n",
    "\n",
    "types = ['RES', 'SAR', 'PRO', 'SSR', 'SCR', 'ESR', 'HSR']\n",
    "combined_df = combined_df[combined_df.type.isin(types)]\n",
    "\n",
    "# create formatted pub number with type and number\n",
    "combined_df['pub_number'] = combined_df['type'] + \" \" + combined_df['pub_number']\n",
    "\n",
    "columns = ['pub_number', 'year', 'nom', 'name', 'url_fr', 'url_en']\n",
    "combined_df = combined_df[columns].reset_index(drop=True)"
   ],
   "id": "8ec682a54b98f832",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T12:31:30.397352Z",
     "start_time": "2025-01-24T12:31:30.372987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# populate pdf filenames where they exist\n",
    "\n",
    "combined_df['filename_fr'] = combined_df['url_fr'].str.split('/').str[-1]\n",
    "combined_df['filename_en'] = combined_df['url_en'].str.split('/').str[-1]\n",
    "combined_df.loc[~combined_df['filename_fr'].str.endswith('.pdf'), 'filename_fr'] = None\n",
    "combined_df.loc[~combined_df['filename_en'].str.endswith('.pdf'), 'filename_en'] = None\n",
    "\n",
    "# add file_url columns\n",
    "combined_df['file_url_fr'] = np.where(combined_df['filename_fr'], combined_df['url_fr'], None)\n",
    "combined_df['file_url_en'] = np.where(combined_df['filename_en'], combined_df['url_en'], None)\n",
    "\n",
    "# display(combined_df[combined_df['filename_fr'].isna()].sample(1).T)\n",
    "# display(combined_df[combined_df['filename_fr'].notna()].sample(1).T)"
   ],
   "id": "2df2d7f2e4c70328",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# import parsed publication url data",
   "id": "d97cc09b896d1e24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T12:31:47.203165Z",
     "start_time": "2025-01-24T12:31:30.443735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parsed_docs_folder = os.path.join(\"..\", \"ParsedPublications\")\n",
    "min_year, max_year = 1977, 2024\n",
    "data = []\n",
    "\n",
    "def process_file(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "        return {\n",
    "            'filename': json_data.get('name'),\n",
    "            'year': json_data.get('publicationYear'),\n",
    "            'url': json_data.get('url'),\n",
    "        }\n",
    "\n",
    "def process_folder(year_path):\n",
    "    file_data = []\n",
    "    for json_file in os.listdir(year_path):\n",
    "        if json_file.endswith(\".json\"):\n",
    "            json_path = os.path.join(year_path, json_file)\n",
    "            file_data.append(process_file(json_path))\n",
    "    return file_data\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for year_folder in os.listdir(parsed_docs_folder):\n",
    "        if year_folder.isnumeric() and min_year <= int(year_folder) <= max_year:\n",
    "            year_path = os.path.join(parsed_docs_folder, year_folder)\n",
    "            if os.path.isdir(year_path):\n",
    "                futures.append(executor.submit(process_folder, year_path))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        data.extend(future.result())\n",
    "\n",
    "parsed_docs_df = pd.DataFrame(data) # this took 3 seconds\n",
    "unmatched_url = set(parsed_docs_df['url'].to_list())\n",
    "\n",
    "# confirm all url are distinct\n",
    "parsed_docs_df['url'].value_counts().value_counts()"
   ],
   "id": "d379bf789ff3b4a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count\n",
       "1    12752\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# crawl bs4 to get remaining filename_fr and filename_en",
   "id": "5e8f706c2ddbf4be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T13:24:49.050902Z",
     "start_time": "2025-01-24T13:24:49.023774Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url_en\n",
       "True     7692\n",
       "False    1369\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "url_en\n",
       "False    7692\n",
       "True     1369\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "url_fr\n",
       "True     7692\n",
       "False    1369\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "url_fr\n",
       "False    7692\n",
       "True     1369\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 56,
   "source": [
    "# what are the different link suffices? -> 'html', 'htm', 'pdf' \n",
    "#  exclude pdf or not htm/html\n",
    "display(combined_df['url_en'].str.endswith(('html', 'htm')).value_counts())\n",
    "display(combined_df['url_en'].str.endswith('pdf').value_counts())\n",
    "display(combined_df['url_fr'].str.endswith(('html', 'htm')).value_counts())\n",
    "display(combined_df['url_fr'].str.endswith('pdf').value_counts())"
   ],
   "id": "85149a9d001531b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T13:21:24.676107Z",
     "start_time": "2025-01-24T13:21:24.657585Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 52,
   "source": [
    "errors = dict()\n",
    "\n",
    "\n",
    "def find_pdf_link(url, debug=False):\n",
    "    global errors\n",
    "    \n",
    "    if url.split('.')[-1].lower() not in ['html', 'htm']:\n",
    "        if debug:\n",
    "            errors['debug_pdf'] = f'{url=}'\n",
    "        return None\n",
    "    \n",
    "    if debug:\n",
    "        errors = dict()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = re.findall(r'http[s]?://[^\\'\"<>]+\\.pdf', str(soup), re.IGNORECASE)\n",
    "        \n",
    "        if debug:\n",
    "            errors['debug_soup'] = soup\n",
    "        if debug:\n",
    "            errors['debug_pdf_links'] = pdf_links\n",
    "            \n",
    "        for link in pdf_links:\n",
    "            if debug:\n",
    "                errors['debug_link'] = link\n",
    "                \n",
    "            if link in unmatched_url:\n",
    "                return link\n",
    "            elif debug:\n",
    "                print(f'{link=} not in unmatched_url')\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        errors[url] = f\"find_pdf_link Error: {e}\"\n",
    "        \n",
    "    return None"
   ],
   "id": "ef95f3d00283876b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T13:38:02.267353Z",
     "start_time": "2025-01-24T13:38:02.258574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# conditionally load combined_df from csv\n",
    "\n",
    "output_file = 'updated_parsed_docs.csv'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_csv(output_file)\n",
    "    print(\"File loaded successfully.\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"{output_file} does not exist.\")"
   ],
   "id": "5f11ea3e62cd1322",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated_parsed_docs.csv does not exist.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "save_batch = True\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    if save_batch:\n",
    "        if index % batch_size == 0:\n",
    "            combined_df.to_csv(output_file, index=False)\n",
    "            print(f\"Progress saved after {index + 1} rows.\")\n",
    "    \n",
    "    if (row['filename_fr'] and row['filename_en']) or (row['file_url_fr'] and row['file_url_en']):\n",
    "        continue\n",
    "    \n",
    "    pdf_link_fr = None\n",
    "    pdf_link_en = None\n",
    "    \n",
    "    if pd.isna(row['filename_fr']):\n",
    "        pdf_link_fr = find_pdf_link(row['url_fr'])\n",
    "        if pdf_link_fr:\n",
    "            combined_df.at[index, 'file_url_fr'] = pdf_link_fr\n",
    "            if pdf_link_fr in unmatched_url:\n",
    "                unmatched_url.discard(pdf_link_fr)\n",
    "            else:\n",
    "                errors[pdf_link_fr] = 'Link was added to combined_df but not in unmatched_url (fr)'\n",
    "\n",
    "    if pd.isna(row['filename_en']):\n",
    "        if row['url_en'] != row['url_fr']:\n",
    "            pdf_link_en = find_pdf_link(row['url_en'])\n",
    "            if pdf_link_en:\n",
    "                combined_df.at[index, 'file_url_en'] = pdf_link_en\n",
    "                if pdf_link_en in unmatched_url:\n",
    "                    unmatched_url.discard(pdf_link_en)\n",
    "                else:\n",
    "                    errors[pdf_link_en] = 'Link was added to combined_df but not in unmatched_url (en)'\n",
    "        else:\n",
    "            if pdf_link_fr:\n",
    "                combined_df.at[index, 'file_url_en'] = pdf_link_fr\n",
    "\n",
    "\n",
    "if save_batch:\n",
    "    combined_df.to_csv(output_file, index=False)"
   ],
   "id": "7368ee5b1293c750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.480341Z",
     "start_time": "2025-01-23T17:16:14.477723Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "271462351ade0c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.506007Z",
     "start_time": "2025-01-23T17:16:14.497336Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "36c2f4bd43b89034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# break into paragraph chunks\n",
    "* do a bit of research first to check what style and format chunks are best for translation finetuning"
   ],
   "id": "b0c0ddbb0f5c9116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.539233Z",
     "start_time": "2025-01-23T17:16:14.536358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: break into paragraph chunks for better correlation \n",
    "#  TODO (OPTIONAL): clean excess characters\n",
    "#  TODO (OPTIONAL): make sure french-friendly encoding is used (at least check if that makes a difference)\n"
   ],
   "id": "fc6383d0be7fd92b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.564438Z",
     "start_time": "2025-01-23T17:16:14.562028Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "41226b0a82be098",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.598852Z",
     "start_time": "2025-01-23T17:16:14.596304Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "70f1e76e3c520246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.622366Z",
     "start_time": "2025-01-23T17:16:14.620526Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3dfc6096b226f6f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T17:16:14.649961Z",
     "start_time": "2025-01-23T17:16:14.646267Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "624a4a0f283826ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
