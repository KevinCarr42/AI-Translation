{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:35.059382Z",
     "start_time": "2025-01-30T16:56:34.641600Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # jupyter notebook full-width display\n",
    "display(HTML(\"<style>.dataframe td { white-space: nowrap; }</style>\")) # no text wrapping\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.dataframe td { white-space: nowrap; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:35.171023Z",
     "start_time": "2025-01-30T16:56:35.088644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all files that have been downloaded and parsed\n",
    "parsed_docs_folder = os.path.join(\"..\", \"ParsedPublications\")\n",
    "\n",
    "min_year = 2023\n",
    "parsed_files = list()\n",
    "parsed_files_with_hq_ocr = list()\n",
    "for folder in os.listdir(parsed_docs_folder):\n",
    "    path = os.path.join(parsed_docs_folder, folder)\n",
    "    if os.path.isdir(path):\n",
    "        for json_file in os.listdir(path):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                parsed_files.append(json_file.replace('.json', ''))\n",
    "                if folder in [str(year) for year in range(min_year, 2024 + 1)]:\n",
    "                    parsed_files_with_hq_ocr.append(json_file.replace('.json', ''))\n",
    "\n",
    "# all files from website\n",
    "fr_eng_correlation_csv = \"fr_eng_correlation_data.csv\"\n",
    "fr_eng_correlation_df = pd.read_csv(fr_eng_correlation_csv)\n",
    "# exclude files that aren't downloaded, and files that have been withdrawn\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[(fr_eng_correlation_df.filename_en.isin(parsed_files)) | (fr_eng_correlation_df.filename_fr.isin(parsed_files))]\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[(fr_eng_correlation_df.filename_en != 'WITHDRAWN') & (fr_eng_correlation_df.filename_fr != 'WITHDRAWN')]\n",
    "\n",
    "# weblinks for previewing / checking results\n",
    "weblinks_df = fr_eng_correlation_df.copy()\n",
    "weblinks_df = weblinks_df[['pub_number', 'nom', 'name', 'url_fr', 'url_en', 'file_url_fr', 'file_url_en']]\n",
    "\n",
    "# data to be used for language classifier\n",
    "lang_df = fr_eng_correlation_df.copy()\n",
    "lang_df = lang_df[(lang_df.filename_fr.isin(parsed_files_with_hq_ocr)) & (lang_df.filename_en.isin(parsed_files_with_hq_ocr)) & (lang_df.filename_fr != lang_df.filename_en)]\n"
   ],
   "id": "83675b54298a8486",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# helper functions",
   "id": "ca8dc9ee3a7f7d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:35.326124Z",
     "start_time": "2025-01-30T16:56:35.320927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preview_publication(pub_number):\n",
    "    if type(pub_number) is pd.DataFrame and pub_number.shape[0] == 1:\n",
    "        try:\n",
    "            pub_number = pub_number['pub_number'].values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    elif type(pub_number) is pd.Series:\n",
    "        try:\n",
    "            pub_number = pub_number.values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        output_df = weblinks_df[weblinks_df.pub_number == pub_number].T\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_filepaths(row, min_year=2023):\n",
    "    fr_filename, en_filename = row['filename_fr'] + '.json', row['filename_en'] + '.json'\n",
    "    file_folders = [os.path.join('..', 'ParsedPublications', str(year)) for year in range(min_year, 2024 + 1)]\n",
    "    \n",
    "    try:\n",
    "        fr_path, en_path = ([os.path.join(folder, fr_filename) for folder in file_folders if os.path.exists(os.path.join(folder, fr_filename))][0], \n",
    "                            [os.path.join(folder, en_filename) for folder in file_folders if os.path.exists(os.path.join(folder, en_filename))][0])\n",
    "    except IndexError:\n",
    "        return None, None\n",
    "    \n",
    "    return fr_path, en_path\n"
   ],
   "id": "93d13f552b93b1a0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:40.454955Z",
     "start_time": "2025-01-30T16:56:35.335067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make lists of all French words and all English words\n",
    "\n",
    "valid_word_regex = re.compile(r'^[a-zA-ZÀ-ÿ]+$')\n",
    "french_word_list = []\n",
    "english_word_list = []\n",
    "exclude_words_with_less_than_n = 10\n",
    "\n",
    "# clean headers and appendices\n",
    "references_fr = r'RÉFÉRENCES CITÉES'.lower()\n",
    "references_en = r'REFERENCES CITED'.lower()\n",
    "\n",
    "\n",
    "for i, row in lang_df.iterrows():\n",
    "    fr_path, en_path = get_filepaths(row)\n",
    "    \n",
    "    with open(fr_path, 'r', encoding='utf-8') as file:\n",
    "        fr_text = json.load(file).get('text', '').lower()\n",
    "            \n",
    "        parts = re.split(references_fr, fr_text, flags=re.IGNORECASE)\n",
    "        if 2 < len(parts) < 5:  # if 2 or 3 occurences of references text, take the second part (to get the main body text)\n",
    "            fr_text = parts[1]\n",
    "        \n",
    "        french_word_list.extend(word for word in fr_text.split() if valid_word_regex.match(word))\n",
    "    \n",
    "    with open(en_path, 'r', encoding='utf-8') as file:\n",
    "        en_text = json.load(file).get('text', '').lower()\n",
    "            \n",
    "        parts = re.split(references_en, en_text, flags=re.IGNORECASE)\n",
    "        if 2 < len(parts) < 5:  # if 2 or 3 occurences of references text, take the second part (to get the main body text)\n",
    "            en_text = parts[1]\n",
    "        \n",
    "        english_word_list.extend(word for word in en_text.split() if valid_word_regex.match(word))\n",
    "        \n",
    "# For testing\n",
    "french_word_counts = Counter(french_word_list)\n",
    "french_word_counts_expanded = []\n",
    "for word, count in french_word_counts.items():\n",
    "    for _ in range(count):\n",
    "        french_word_counts_expanded.append((word, count))\n",
    "        \n",
    "english_word_counts = Counter(english_word_list)\n",
    "english_word_counts_expanded = []\n",
    "for word, count in english_word_counts.items():\n",
    "    for _ in range(count):\n",
    "        english_word_counts_expanded.append((word, count))\n",
    "\n",
    "full_french_word_list = french_word_list.copy()\n",
    "full_english_word_list = english_word_list.copy()\n",
    "\n",
    "# Remove words with less than 10 occurrences\n",
    "french_word_list = [word for word, count in french_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "english_word_list = [word for word, count in english_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "\n",
    "# Convert to sets for further processing\n",
    "french_words = set(french_word_list)\n",
    "english_words = set(english_word_list)\n",
    "\n",
    "# Remove overlapping words\n",
    "overlapping_words = english_words & french_words\n",
    "english_words.difference_update(overlapping_words)\n",
    "french_words.difference_update(overlapping_words)\n",
    "\n",
    "# Remove numeric-only words\n",
    "english_words.difference_update({w for w in english_words if w.isnumeric()})\n",
    "french_words.difference_update({w for w in french_words if w.isnumeric()})\n"
   ],
   "id": "713c097efeb1dc20",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:40.544401Z",
     "start_time": "2025-01-30T16:56:40.537003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper functions for word lists\n",
    "\n",
    "def test_wordlists(text_block, english_words, french_words):\n",
    "    en_count = sum(1 for word in text_block.split() if word in english_words)\n",
    "    fr_count = sum(1 for word in text_block.split() if word in french_words)\n",
    "    \n",
    "    print('english words:', list(word for word in text_block.split() if word in english_words))\n",
    "    print('french words:', list(word for word in text_block.split() if word in french_words))\n",
    "    print(f'{en_count=}, {fr_count=}')    \n",
    "    \n",
    "def most_common_word_info(counter_obj, n=10):  \n",
    "    length = counter_obj.total()\n",
    "    c_v = 0\n",
    "    for k, v in counter_obj.most_common(n):\n",
    "        rng = f'({(100 * c_v) / length:.0f}%'\n",
    "        c_v += v\n",
    "        rng += f'-{(100 * c_v) / length:.0f}%)'\n",
    "        print(f'{k:<20}{v:>8}{(100 * v) / length:>8.0f}%{rng:>15}')\n",
    "\n",
    "def nth_percentile(p, counter_obj, greater_than=True):\n",
    "    sorted_list = sorted(counter_obj.items(), key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return sorted_list[index]\n",
    "\n",
    "def nth_percentile_weighted(p, counter_expanded, greater_than=True):\n",
    "    sorted_list = sorted(counter_expanded, key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return sorted_list[index]\n",
    "\n",
    "def count_nth_percentile(p, counter_obj, greater_than=True):\n",
    "    sorted_list = sorted(counter_obj.items(), key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return len(sorted_list[index:]) if greater_than else len(sorted_list[-max(index, 1):])\n",
    "\n",
    "def count_nth_percentile_weighted(p, counter_expanded, greater_than=True):\n",
    "    sorted_list = sorted(counter_expanded, key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    count_if_gte = sorted_list[len(sorted_list) - index][1]\n",
    "    count_if_lte = sorted_list[index][1]\n",
    "        \n",
    "    gte = {x for x in counter_expanded if x[1] >= count_if_gte}\n",
    "    lte = {x for x in counter_expanded if x[1] <= count_if_lte}\n",
    "    \n",
    "    return len(gte) if greater_than else len(lte)\n",
    "\n",
    "def examples_at_word_count(n, counter_obj, n_samples=5, tolerance=0.1):\n",
    "    lower, upper = int((1 - 0.1) * n), int((1 + 0.1) * n)\n",
    "    all_examples = [k for k, v in counter_obj.items() if lower <= v <= upper]\n",
    "    \n",
    "    return random.sample(all_examples, min(n_samples, len(all_examples)))\n"
   ],
   "id": "83648ca204fa96b0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:40.589045Z",
     "start_time": "2025-01-30T16:56:40.579963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_wordlists('Total mortalities at age, based on survey data, are presented in Table', english_words, french_words)\n",
    "print()\n",
    "most_common_word_info(english_word_counts, 4)\n",
    "print()\n",
    "most_common_word_info(french_word_counts, 4)"
   ],
   "id": "4b842b447ae4f093",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words: ['mortalities', 'presented']\n",
      "french words: []\n",
      "en_count=2, fr_count=0\n",
      "\n",
      "the                   168894       7%        (0%-7%)\n",
      "of                     94029       4%       (7%-11%)\n",
      "and                    93778       4%      (11%-15%)\n",
      "in                     72480       3%      (15%-19%)\n",
      "\n",
      "de                    237366       8%        (0%-8%)\n",
      "la                    140133       5%       (8%-13%)\n",
      "et                    100646       4%      (13%-17%)\n",
      "les                   100095       4%      (17%-21%)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:41.460762Z",
     "start_time": "2025-01-30T16:56:40.668361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.1, 0.5, 0.95]:\n",
    "    print(p, nth_percentile(p, english_word_counts), nth_percentile(p, english_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, english_word_counts_expanded), nth_percentile_weighted(p, english_word_counts_expanded, False))\n",
    "\n",
    "print()\n",
    "\n",
    "for n in [1, 10, 100, 1000, 10000]:\n",
    "    print(n, examples_at_word_count(n, english_word_counts))"
   ],
   "id": "57387b89a4c41acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 ('endemism', 87) ('terebratulina', 1)\n",
      "0.1 weighted ('of', 94029) ('adolescent', 99)\n",
      "0.5 ('contradiction', 3) ('hshlf', 3)\n",
      "0.5 weighted ('landings', 2571) ('landings', 2571)\n",
      "0.95 ('biron', 1) ('linear', 238)\n",
      "0.95 weighted ('cobble', 34) ('the', 168894)\n",
      "\n",
      "1 ['franceschini', 'abell', 'lodge', 'geosci', 'medusarum']\n",
      "10 ['sheila', 'footage', 'stepwise', 'compound', 'huron']\n",
      "100 ['cage', 'y', 'adolescent', 'charlotte', 'recognized']\n",
      "1000 ['redfish', 'collected', 'program', 'bc', 'part']\n",
      "10000 ['not', 'at', 'data']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:42.590578Z",
     "start_time": "2025-01-30T16:56:41.501405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.001, 0.01, .99, .999]:\n",
    "    print(p, nth_percentile(p, english_word_counts), nth_percentile(p, english_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, english_word_counts_expanded), nth_percentile_weighted(p, english_word_counts_expanded, False))"
   ],
   "id": "8ea7020297359e47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ('an', 7322) ('skipper', 1)\n",
      "0.001 weighted ('the', 168894) ('steedman', 1)\n",
      "0.01 ('indicators', 1239) ('firefighting', 1)\n",
      "0.01 weighted ('the', 168894) ('myxine', 4)\n",
      "0.99 ('cognition', 1) ('indicators', 1239)\n",
      "0.99 weighted ('visualizing', 4) ('the', 168894)\n",
      "0.999 ('jacking', 1) ('an', 7322)\n",
      "0.999 weighted ('bloch', 1) ('the', 168894)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:43.698401Z",
     "start_time": "2025-01-30T16:56:42.637289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.1, 0.5, 0.95]:\n",
    "    print(p, nth_percentile(p, french_word_counts), nth_percentile(p, french_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, french_word_counts_expanded), nth_percentile_weighted(p, french_word_counts_expanded, False))\n",
    "\n",
    "print()\n",
    "\n",
    "for n in [1, 10, 100, 1000, 10000]:\n",
    "    print(n, examples_at_word_count(n, french_word_counts))"
   ],
   "id": "71663313fedcb0ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 ('appelée', 64) ('héberge', 1)\n",
      "0.1 weighted ('la', 140133) ('estivale', 96)\n",
      "0.5 ('cubiques', 3) ('résistent', 3)\n",
      "0.5 weighted ('ne', 5866) ('ne', 5866)\n",
      "0.95 ('interagency', 1) ('conseil', 172)\n",
      "0.95 weighted ('simard', 32) ('de', 237366)\n",
      "\n",
      "1 ['invoquait', 'citez', 'neighbour', 'bessel', 'mésotrophes']\n",
      "10 ['viser', 'inscription', 'chapter', 'reports', 'consister']\n",
      "100 ['chaîne', 'thompson', 'indirects', 'infratidale', 'décalage']\n",
      "1000 ['aussi', 'fois', 'changement', 'leurs', 'indiquent']\n",
      "10000 ['pas', 'aux', 'ou', 'avec']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:45.150907Z",
     "start_time": "2025-01-30T16:56:43.745906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.001, 0.01, .99, .999]:\n",
    "    print(p, nth_percentile(p, french_word_counts), nth_percentile(p, french_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, french_word_counts_expanded), nth_percentile_weighted(p, french_word_counts_expanded, False))"
   ],
   "id": "af94be3f9391a85c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ('il', 8012) ('convoquer', 1)\n",
      "0.001 weighted ('de', 237366) ('plivelic', 1)\n",
      "0.01 ('différentes', 954) ('opérateurs', 1)\n",
      "0.01 weighted ('de', 237366) ('synonyme', 4)\n",
      "0.99 ('fréquentations', 1) ('différentes', 954)\n",
      "0.99 weighted ('rein', 4) ('de', 237366)\n",
      "0.999 ('retirez', 1) ('il', 8012)\n",
      "0.999 weighted ('strub', 1) ('de', 237366)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:45.203623Z",
     "start_time": "2025-01-30T16:56:45.195581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(text, references_pattern):\n",
    "    valid_word_regex = re.compile(r'^[a-zA-ZÀ-ÿ]+$')\n",
    "    min_length, max_length = 5, 20\n",
    "    \n",
    "    document_parts = re.split(references_pattern, text, flags=re.IGNORECASE)\n",
    "    if 2 < len(document_parts) < 5:\n",
    "        text = document_parts[1]\n",
    "    word_list = [word for word in text.split() if valid_word_regex.match(word)]\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^a-zA-ZÀ-ÿ.\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    sentence_list = [\n",
    "        sentence.strip() for sentence in cleaned_text.split('.')\n",
    "        if min_length <= len(sentence.split()) <= max_length\n",
    "        and all(valid_word_regex.match(word) for word in sentence.split())\n",
    "    ]\n",
    "    \n",
    "    return word_list, sentence_list\n",
    "\n",
    "\n",
    "def generate_word_lists(n):\n",
    "    french_word_list = []\n",
    "    english_word_list = []\n",
    "    exclude_words_with_less_than_n = n\n",
    "    \n",
    "    # example sentences\n",
    "    french_example_sentences = []\n",
    "    english_example_sentences = []\n",
    "    \n",
    "    references_fr = r'RÉFÉRENCES CITÉES'.lower()\n",
    "    references_en = r'REFERENCES CITED'.lower()\n",
    "        \n",
    "    for i, row in lang_df.iterrows():\n",
    "        fr_path, en_path = get_filepaths(row)\n",
    "        \n",
    "        with open(fr_path, 'r', encoding='utf-8') as file:\n",
    "            fr_text = json.load(file).get('text', '').lower()\n",
    "            word_list, sentence_list = process_text(fr_text, references_fr)\n",
    "            french_word_list.extend(word_list)\n",
    "            french_example_sentences.extend(sentence_list)\n",
    "        \n",
    "        with open(en_path, 'r', encoding='utf-8') as file:\n",
    "            en_text = json.load(file).get('text', '').lower()\n",
    "            word_list, sentence_list = process_text(en_text, references_en)\n",
    "            english_word_list.extend(word_list)\n",
    "            english_example_sentences.extend(sentence_list)\n",
    "            \n",
    "    # Remove words with less than 10 occurrences\n",
    "    french_word_list = [word for word, count in french_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "    english_word_list = [word for word, count in english_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "    \n",
    "    # Convert to sets for further processing\n",
    "    french_words = set(french_word_list)\n",
    "    english_words = set(english_word_list)\n",
    "    \n",
    "    # Remove overlapping words\n",
    "    overlapping_words = english_words & french_words\n",
    "    english_words.difference_update(overlapping_words)\n",
    "    french_words.difference_update(overlapping_words)\n",
    "    \n",
    "    # Remove numeric-only words\n",
    "    english_words.difference_update({w for w in english_words if w.isnumeric()})\n",
    "    french_words.difference_update({w for w in french_words if w.isnumeric()})\n",
    "    \n",
    "    return english_words, french_words, french_example_sentences, english_example_sentences\n"
   ],
   "id": "d61afdf21f22c367",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:56:45.267794Z",
     "start_time": "2025-01-30T16:56:45.250023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# good start for hyperparam(s)\n",
    "for n in [0, 1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    print(n, sum([1 for x in french_word_counts.values() if x > n]), sum([1 for x in english_word_counts.values() if x > n]))"
   ],
   "id": "da17c0a6bc8a47aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31713 25194\n",
      "1 21977 17713\n",
      "5 12428 10323\n",
      "10 9087 7741\n",
      "20 6304 5591\n",
      "50 3716 3492\n",
      "100 2327 2301\n",
      "200 1397 1436\n",
      "500 618 657\n",
      "1000 298 319\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:57:36.623149Z",
     "start_time": "2025-01-30T16:56:45.321867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_results(n_to_exclude, n_trials):\n",
    "    results = []\n",
    "    \n",
    "    _, _, french_example_sentences, english_example_sentences = generate_word_lists(0)\n",
    "    french_example_sentences_n = random.sample(french_example_sentences, n_trials)\n",
    "    english_example_sentences_n = random.sample(english_example_sentences, n_trials)\n",
    "    \n",
    "    for n in n_to_exclude:\n",
    "        print(f'Processing {n}')\n",
    "        english_words, french_words, french_example_sentences, english_example_sentences = generate_word_lists(n)\n",
    "        \n",
    "        for sentence in french_example_sentences_n:\n",
    "            fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "            en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "            results.append((n, 'fr', fr_count, en_count))\n",
    "        \n",
    "        for sentence in english_example_sentences_n:\n",
    "            fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "            en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "            results.append((n, 'en', fr_count, en_count))\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.columns = ['n_excluded', 'language', 'fr_count', 'en_count']\n",
    "    \n",
    "    results_df['total_count'] = results_df['fr_count'] + results_df['en_count']\n",
    "    \n",
    "    valid_mask = results_df['total_count'] > 0\n",
    "    \n",
    "    results_df['correct_count'] = np.where(\n",
    "        (results_df['language'] == 'fr') & valid_mask,\n",
    "        results_df['fr_count'],\n",
    "        np.where(\n",
    "            (results_df['language'] == 'en') & valid_mask,\n",
    "            results_df['en_count'],\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results_df['wrong_count'] = np.where(\n",
    "        (results_df['language'] == 'fr') & valid_mask,\n",
    "        results_df['en_count'],\n",
    "        np.where(\n",
    "            (results_df['language'] == 'en') & valid_mask,\n",
    "            results_df['fr_count'],\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results_df['is_correct'] = results_df['correct_count'] > results_df['wrong_count']\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "n_to_exclude = [0, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "n_trials = 100\n",
    "results_df = process_results(n_to_exclude, n_trials)\n"
   ],
   "id": "7120e7465f02ff17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 1\n",
      "Processing 5\n",
      "Processing 10\n",
      "Processing 20\n",
      "Processing 50\n",
      "Processing 100\n",
      "Processing 200\n",
      "Processing 500\n",
      "Processing 1000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:57:36.683642Z",
     "start_time": "2025-01-30T16:57:36.666652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STATS\n",
    "\n",
    "def create_stats(results_df):\n",
    "    \n",
    "    # Define FP & FN for each language\n",
    "    results_df['fr_false_positive'] = (results_df['language'] == 'en') & (results_df['is_correct'] == False)\n",
    "    results_df['fr_false_negative'] = (results_df['language'] == 'fr') & (results_df['is_correct'] == False)\n",
    "    results_df['en_false_positive'] = (results_df['language'] == 'fr') & (results_df['is_correct'] == False)\n",
    "    results_df['en_false_negative'] = (results_df['language'] == 'en') & (results_df['is_correct'] == False)\n",
    "    \n",
    "    # Compute separate aggregations\n",
    "    grouped_df = results_df.groupby('n_excluded').agg(\n",
    "        total_count=('is_correct', 'count'),  # count rows (not the same as results_df['total_count']\n",
    "        \n",
    "        # Correct and incorrect classifications\n",
    "        correct_count=('is_correct', 'sum'),\n",
    "        wrong_count=('is_correct', lambda x: (~x).sum()),  \n",
    "    \n",
    "        # False Positives & False Negatives for each language\n",
    "        fr_false_positive=('fr_false_positive', 'sum'),\n",
    "        fr_false_negative=('fr_false_negative', 'sum'),\n",
    "        en_false_positive=('en_false_positive', 'sum'),\n",
    "        en_false_negative=('en_false_negative', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    valid_mask = grouped_df['total_count'] > 0\n",
    "    \n",
    "    # Accuracy (same for both languages)\n",
    "    grouped_df['accuracy'] = np.where(valid_mask, grouped_df['correct_count'] / grouped_df['total_count'], 0)\n",
    "    \n",
    "    # Precision & Recall for French\n",
    "    fr_precision_mask = (grouped_df['correct_count'] + grouped_df['fr_false_positive']) > 0\n",
    "    grouped_df['fr_precision'] = np.where(\n",
    "        fr_precision_mask, \n",
    "        grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['fr_false_positive']), \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    fr_recall_mask = (grouped_df['correct_count'] + grouped_df['fr_false_negative']) > 0\n",
    "    grouped_df['fr_recall'] = np.where(\n",
    "        fr_recall_mask, \n",
    "        grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['fr_false_negative']), \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Precision & Recall for English\n",
    "    en_precision_mask = (grouped_df['correct_count'] + grouped_df['en_false_positive']) > 0\n",
    "    grouped_df['en_precision'] = np.where(\n",
    "        en_precision_mask, \n",
    "        grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['en_false_positive']), \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    en_recall_mask = (grouped_df['correct_count'] + grouped_df['en_false_negative']) > 0\n",
    "    grouped_df['en_recall'] = np.where(\n",
    "        en_recall_mask, \n",
    "        grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['en_false_negative']), \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # F1-scores\n",
    "    grouped_df['fr_f1_score'] = np.where(\n",
    "        (grouped_df['fr_precision'] + grouped_df['fr_recall']) > 0,\n",
    "        2 * (grouped_df['fr_precision'] * grouped_df['fr_recall']) / (grouped_df['fr_precision'] + grouped_df['fr_recall']),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    grouped_df['en_f1_score'] = np.where(\n",
    "        (grouped_df['en_precision'] + grouped_df['en_recall']) > 0,\n",
    "        2 * (grouped_df['en_precision'] * grouped_df['en_recall']) / (grouped_df['en_precision'] + grouped_df['en_recall']),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    grouped_df['stats_sum'] = grouped_df[['accuracy', 'fr_precision', 'fr_recall', 'en_precision', 'en_recall', 'fr_f1_score', 'en_f1_score']].sum(axis=1)\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "\n",
    "grouped_df = create_stats(results_df)\n"
   ],
   "id": "ce8ea2c057ffe7fc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:57:36.739902Z",
     "start_time": "2025-01-30T16:57:36.729667Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_df.set_index('n_excluded').T",
   "id": "cb44d9b835742b3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded          0      1      5      10     20     50     100    200   \\\n",
       "total_count       200.00 200.00 200.00 200.00 200.00 200.00 200.00 200.00   \n",
       "correct_count     157.00 157.00 175.00 184.00 185.00 185.00 188.00 187.00   \n",
       "wrong_count        43.00  43.00  25.00  16.00  15.00  15.00  12.00  13.00   \n",
       "fr_false_positive  28.00  28.00  13.00   5.00   4.00   4.00   2.00   3.00   \n",
       "fr_false_negative  15.00  15.00  12.00  11.00  11.00  11.00  10.00  10.00   \n",
       "en_false_positive  15.00  15.00  12.00  11.00  11.00  11.00  10.00  10.00   \n",
       "en_false_negative  28.00  28.00  13.00   5.00   4.00   4.00   2.00   3.00   \n",
       "accuracy            0.79   0.79   0.88   0.92   0.93   0.93   0.94   0.94   \n",
       "fr_precision        0.85   0.85   0.93   0.97   0.98   0.98   0.99   0.98   \n",
       "fr_recall           0.91   0.91   0.94   0.94   0.94   0.94   0.95   0.95   \n",
       "en_precision        0.91   0.91   0.94   0.94   0.94   0.94   0.95   0.95   \n",
       "en_recall           0.85   0.85   0.93   0.97   0.98   0.98   0.99   0.98   \n",
       "fr_f1_score         0.88   0.88   0.93   0.96   0.96   0.96   0.97   0.97   \n",
       "en_f1_score         0.88   0.88   0.93   0.96   0.96   0.96   0.97   0.97   \n",
       "stats_sum           6.07   6.07   6.48   6.67   6.69   6.69   6.76   6.73   \n",
       "\n",
       "n_excluded          500    1000  \n",
       "total_count       200.00 200.00  \n",
       "correct_count     188.00 187.00  \n",
       "wrong_count        12.00  13.00  \n",
       "fr_false_positive   2.00   3.00  \n",
       "fr_false_negative  10.00  10.00  \n",
       "en_false_positive  10.00  10.00  \n",
       "en_false_negative   2.00   3.00  \n",
       "accuracy            0.94   0.94  \n",
       "fr_precision        0.99   0.98  \n",
       "fr_recall           0.95   0.95  \n",
       "en_precision        0.95   0.95  \n",
       "en_recall           0.99   0.98  \n",
       "fr_f1_score         0.97   0.97  \n",
       "en_f1_score         0.97   0.97  \n",
       "stats_sum           6.76   6.73  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_excluded</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>500</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_count</th>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_count</th>\n",
       "      <td>157.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>175.00</td>\n",
       "      <td>184.00</td>\n",
       "      <td>185.00</td>\n",
       "      <td>185.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>187.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>187.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong_count</th>\n",
       "      <td>43.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_positive</th>\n",
       "      <td>28.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_negative</th>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_positive</th>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_negative</th>\n",
       "      <td>28.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_precision</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_recall</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_precision</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_recall</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_f1_score</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_f1_score</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats_sum</th>\n",
       "      <td>6.07</td>\n",
       "      <td>6.07</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T17:02:31.049007Z",
     "start_time": "2025-01-30T17:02:31.042542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all stats added together\n",
    "grouped_df.set_index('n_excluded')['stats_sum']"
   ],
   "id": "269c750da1846626",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded\n",
       "0      6.07\n",
       "1      6.07\n",
       "5      6.48\n",
       "10     6.67\n",
       "20     6.69\n",
       "50     6.69\n",
       "100    6.76\n",
       "200    6.73\n",
       "500    6.76\n",
       "1000   6.73\n",
       "Name: stats_sum, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:57:36.945562Z",
     "start_time": "2025-01-30T16:57:36.937365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backup old dfs\n",
    "results_df_BACKUP = results_df.copy() \n",
    "grouped_df_BACKUP = grouped_df.copy() "
   ],
   "id": "36cb01715f31204d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:59:20.113836Z",
     "start_time": "2025-01-30T16:57:37.034122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_to_exclude = [x for x in range(50, 550, 50)] + [x for x in range(600, 1100, 100)] + [x for x in range(1200, 2200, 200)]\n",
    "n_trials = 1000\n",
    "\n",
    "results_df = process_results(n_to_exclude, n_trials)\n",
    "grouped_df = create_stats(results_df)"
   ],
   "id": "ba47a7f465f97437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50\n",
      "Processing 100\n",
      "Processing 150\n",
      "Processing 200\n",
      "Processing 250\n",
      "Processing 300\n",
      "Processing 350\n",
      "Processing 400\n",
      "Processing 450\n",
      "Processing 500\n",
      "Processing 600\n",
      "Processing 700\n",
      "Processing 800\n",
      "Processing 900\n",
      "Processing 1000\n",
      "Processing 1200\n",
      "Processing 1400\n",
      "Processing 1600\n",
      "Processing 1800\n",
      "Processing 2000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:59:20.194012Z",
     "start_time": "2025-01-30T16:59:20.183391Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_df.set_index('n_excluded').T",
   "id": "7ef88cd1369058c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded           50      100     150     200     250     300     350   \\\n",
       "total_count       2000.00 2000.00 2000.00 2000.00 2000.00 2000.00 2000.00   \n",
       "correct_count     1854.00 1868.00 1871.00 1861.00 1860.00 1854.00 1856.00   \n",
       "wrong_count        146.00  132.00  129.00  139.00  140.00  146.00  144.00   \n",
       "fr_false_positive   25.00   14.00   13.00   24.00   26.00   30.00   31.00   \n",
       "fr_false_negative  121.00  118.00  116.00  115.00  114.00  116.00  113.00   \n",
       "en_false_positive  121.00  118.00  116.00  115.00  114.00  116.00  113.00   \n",
       "en_false_negative   25.00   14.00   13.00   24.00   26.00   30.00   31.00   \n",
       "accuracy             0.93    0.93    0.94    0.93    0.93    0.93    0.93   \n",
       "fr_precision         0.99    0.99    0.99    0.99    0.99    0.98    0.98   \n",
       "fr_recall            0.94    0.94    0.94    0.94    0.94    0.94    0.94   \n",
       "en_precision         0.94    0.94    0.94    0.94    0.94    0.94    0.94   \n",
       "en_recall            0.99    0.99    0.99    0.99    0.99    0.98    0.98   \n",
       "fr_f1_score          0.96    0.97    0.97    0.96    0.96    0.96    0.96   \n",
       "en_f1_score          0.96    0.97    0.97    0.96    0.96    0.96    0.96   \n",
       "stats_sum            6.70    6.73    6.74    6.72    6.71    6.70    6.71   \n",
       "\n",
       "n_excluded           400     450     500     600     700     800     900   \\\n",
       "total_count       2000.00 2000.00 2000.00 2000.00 2000.00 2000.00 2000.00   \n",
       "correct_count     1854.00 1853.00 1855.00 1854.00 1852.00 1849.00 1845.00   \n",
       "wrong_count        146.00  147.00  145.00  146.00  148.00  151.00  155.00   \n",
       "fr_false_positive   32.00   33.00   31.00   34.00   37.00   39.00   42.00   \n",
       "fr_false_negative  114.00  114.00  114.00  112.00  111.00  112.00  113.00   \n",
       "en_false_positive  114.00  114.00  114.00  112.00  111.00  112.00  113.00   \n",
       "en_false_negative   32.00   33.00   31.00   34.00   37.00   39.00   42.00   \n",
       "accuracy             0.93    0.93    0.93    0.93    0.93    0.92    0.92   \n",
       "fr_precision         0.98    0.98    0.98    0.98    0.98    0.98    0.98   \n",
       "fr_recall            0.94    0.94    0.94    0.94    0.94    0.94    0.94   \n",
       "en_precision         0.94    0.94    0.94    0.94    0.94    0.94    0.94   \n",
       "en_recall            0.98    0.98    0.98    0.98    0.98    0.98    0.98   \n",
       "fr_f1_score          0.96    0.96    0.96    0.96    0.96    0.96    0.96   \n",
       "en_f1_score          0.96    0.96    0.96    0.96    0.96    0.96    0.96   \n",
       "stats_sum            6.70    6.70    6.70    6.70    6.70    6.69    6.68   \n",
       "\n",
       "n_excluded           1000    1200    1400    1600    1800    2000  \n",
       "total_count       2000.00 2000.00 2000.00 2000.00 2000.00 2000.00  \n",
       "correct_count     1844.00 1837.00 1823.00 1820.00 1815.00 1813.00  \n",
       "wrong_count        156.00  163.00  177.00  180.00  185.00  187.00  \n",
       "fr_false_positive   43.00   51.00   66.00   69.00   75.00   77.00  \n",
       "fr_false_negative  113.00  112.00  111.00  111.00  110.00  110.00  \n",
       "en_false_positive  113.00  112.00  111.00  111.00  110.00  110.00  \n",
       "en_false_negative   43.00   51.00   66.00   69.00   75.00   77.00  \n",
       "accuracy             0.92    0.92    0.91    0.91    0.91    0.91  \n",
       "fr_precision         0.98    0.97    0.97    0.96    0.96    0.96  \n",
       "fr_recall            0.94    0.94    0.94    0.94    0.94    0.94  \n",
       "en_precision         0.94    0.94    0.94    0.94    0.94    0.94  \n",
       "en_recall            0.98    0.97    0.97    0.96    0.96    0.96  \n",
       "fr_f1_score          0.96    0.96    0.95    0.95    0.95    0.95  \n",
       "en_f1_score          0.96    0.96    0.95    0.95    0.95    0.95  \n",
       "stats_sum            6.68    6.66    6.63    6.63    6.62    6.61  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_excluded</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>150</th>\n",
       "      <th>200</th>\n",
       "      <th>250</th>\n",
       "      <th>300</th>\n",
       "      <th>350</th>\n",
       "      <th>400</th>\n",
       "      <th>450</th>\n",
       "      <th>500</th>\n",
       "      <th>600</th>\n",
       "      <th>700</th>\n",
       "      <th>800</th>\n",
       "      <th>900</th>\n",
       "      <th>1000</th>\n",
       "      <th>1200</th>\n",
       "      <th>1400</th>\n",
       "      <th>1600</th>\n",
       "      <th>1800</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_count</th>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_count</th>\n",
       "      <td>1854.00</td>\n",
       "      <td>1868.00</td>\n",
       "      <td>1871.00</td>\n",
       "      <td>1861.00</td>\n",
       "      <td>1860.00</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>1856.00</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>1853.00</td>\n",
       "      <td>1855.00</td>\n",
       "      <td>1854.00</td>\n",
       "      <td>1852.00</td>\n",
       "      <td>1849.00</td>\n",
       "      <td>1845.00</td>\n",
       "      <td>1844.00</td>\n",
       "      <td>1837.00</td>\n",
       "      <td>1823.00</td>\n",
       "      <td>1820.00</td>\n",
       "      <td>1815.00</td>\n",
       "      <td>1813.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong_count</th>\n",
       "      <td>146.00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>129.00</td>\n",
       "      <td>139.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>146.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>146.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>145.00</td>\n",
       "      <td>146.00</td>\n",
       "      <td>148.00</td>\n",
       "      <td>151.00</td>\n",
       "      <td>155.00</td>\n",
       "      <td>156.00</td>\n",
       "      <td>163.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>185.00</td>\n",
       "      <td>187.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_positive</th>\n",
       "      <td>25.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_negative</th>\n",
       "      <td>121.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_positive</th>\n",
       "      <td>121.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>113.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>111.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_negative</th>\n",
       "      <td>25.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>51.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_recall</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_precision</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_f1_score</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_f1_score</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats_sum</th>\n",
       "      <td>6.70</td>\n",
       "      <td>6.73</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.71</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.71</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.66</td>\n",
       "      <td>6.63</td>\n",
       "      <td>6.63</td>\n",
       "      <td>6.62</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:59:20.264309Z",
     "start_time": "2025-01-30T16:59:20.258410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all stats added together\n",
    "grouped_df.set_index('n_excluded')['stats_sum']"
   ],
   "id": "b278789fc3d6193a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded\n",
       "50     6.70\n",
       "100    6.73\n",
       "150    6.74\n",
       "200    6.72\n",
       "250    6.71\n",
       "300    6.70\n",
       "350    6.71\n",
       "400    6.70\n",
       "450    6.70\n",
       "500    6.70\n",
       "600    6.70\n",
       "700    6.70\n",
       "800    6.69\n",
       "900    6.68\n",
       "1000   6.68\n",
       "1200   6.66\n",
       "1400   6.63\n",
       "1600   6.63\n",
       "1800   6.62\n",
       "2000   6.61\n",
       "Name: stats_sum, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T17:05:01.682511Z",
     "start_time": "2025-01-30T17:03:26.603322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's check again to see if it's still similar\n",
    "results_df = process_results(n_to_exclude, n_trials)\n",
    "grouped_df = create_stats(results_df)"
   ],
   "id": "c9bf32cd07687f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50\n",
      "Processing 100\n",
      "Processing 150\n",
      "Processing 200\n",
      "Processing 250\n",
      "Processing 300\n",
      "Processing 350\n",
      "Processing 400\n",
      "Processing 450\n",
      "Processing 500\n",
      "Processing 600\n",
      "Processing 700\n",
      "Processing 800\n",
      "Processing 900\n",
      "Processing 1000\n",
      "Processing 1200\n",
      "Processing 1400\n",
      "Processing 1600\n",
      "Processing 1800\n",
      "Processing 2000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T17:05:01.691249Z",
     "start_time": "2025-01-30T17:05:01.682511Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_df.set_index('n_excluded')['stats_sum']",
   "id": "1210b04ef632b07c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded\n",
       "50     6.77\n",
       "100    6.79\n",
       "150    6.79\n",
       "200    6.77\n",
       "250    6.77\n",
       "300    6.77\n",
       "350    6.77\n",
       "400    6.76\n",
       "450    6.76\n",
       "500    6.74\n",
       "600    6.75\n",
       "700    6.74\n",
       "800    6.73\n",
       "900    6.75\n",
       "1000   6.75\n",
       "1200   6.72\n",
       "1400   6.71\n",
       "1600   6.70\n",
       "1800   6.69\n",
       "2000   6.68\n",
       "Name: stats_sum, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T17:08:22.676818Z",
     "start_time": "2025-01-30T17:06:49.991199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tighter hyperparams, more trials\n",
    "grouped_df = create_stats(process_results([x for x in range(50, 525, 25)], 5000))\n",
    "grouped_df.set_index('n_excluded')['stats_sum']"
   ],
   "id": "724548838a486e3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50\n",
      "Processing 75\n",
      "Processing 100\n",
      "Processing 125\n",
      "Processing 150\n",
      "Processing 175\n",
      "Processing 200\n",
      "Processing 225\n",
      "Processing 250\n",
      "Processing 275\n",
      "Processing 300\n",
      "Processing 325\n",
      "Processing 350\n",
      "Processing 375\n",
      "Processing 400\n",
      "Processing 425\n",
      "Processing 450\n",
      "Processing 475\n",
      "Processing 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_excluded\n",
       "50    6.72\n",
       "75    6.74\n",
       "100   6.74\n",
       "125   6.74\n",
       "150   6.75\n",
       "175   6.75\n",
       "200   6.74\n",
       "225   6.74\n",
       "250   6.74\n",
       "275   6.74\n",
       "300   6.74\n",
       "325   6.75\n",
       "350   6.75\n",
       "375   6.75\n",
       "400   6.75\n",
       "425   6.74\n",
       "450   6.74\n",
       "475   6.74\n",
       "500   6.73\n",
       "Name: stats_sum, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Data For Classifier",
   "id": "413fb2ce54ceac84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:27:21.375812Z",
     "start_time": "2025-01-30T18:27:16.708568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "english_words, french_words, french_example_sentences, english_example_sentences = generate_word_lists(150)\n",
    "\n",
    "# save optimised word lists \n",
    "with open(\"language_classifier/wordlists.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"en\": list(english_words), \"fr\": list(french_words)}, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# save 10k sentences per language for testing\n",
    "with open(\"language_classifier/example_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"en\": random.sample(english_example_sentences, 10000), \"fr\": random.sample(french_example_sentences, 10000)}, f, ensure_ascii=False, indent=4)"
   ],
   "id": "b92f2debd2a73216",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "408b7351a871791a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b4f1918d4be6fbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
