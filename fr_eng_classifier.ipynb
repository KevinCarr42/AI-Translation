{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T15:29:35.250931Z",
     "start_time": "2025-01-30T15:29:35.245619Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # jupyter notebook full-width display\n",
    "display(HTML(\"<style>.dataframe td { white-space: nowrap; }</style>\")) # no text wrapping\n",
    "\n",
    "# pandas formatting\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.dataframe td { white-space: nowrap; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:42:52.267865Z",
     "start_time": "2025-01-30T13:42:51.966090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all files that have been downloaded and parsed\n",
    "parsed_docs_folder = os.path.join(\"..\", \"ParsedPublications\")\n",
    "\n",
    "min_year = 2023\n",
    "parsed_files = list()\n",
    "parsed_files_with_hq_ocr = list()\n",
    "for folder in os.listdir(parsed_docs_folder):\n",
    "    path = os.path.join(parsed_docs_folder, folder)\n",
    "    if os.path.isdir(path):\n",
    "        for json_file in os.listdir(path):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                parsed_files.append(json_file.replace('.json', ''))\n",
    "                if folder in [str(year) for year in range(min_year, 2024 + 1)]:\n",
    "                    parsed_files_with_hq_ocr.append(json_file.replace('.json', ''))\n",
    "\n",
    "# all files from website\n",
    "fr_eng_correlation_csv = \"fr_eng_correlation_data.csv\"\n",
    "fr_eng_correlation_df = pd.read_csv(fr_eng_correlation_csv)\n",
    "# exclude files that aren't downloaded, and files that have been withdrawn\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[(fr_eng_correlation_df.filename_en.isin(parsed_files)) | (fr_eng_correlation_df.filename_fr.isin(parsed_files))]\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[(fr_eng_correlation_df.filename_en != 'WITHDRAWN') & (fr_eng_correlation_df.filename_fr != 'WITHDRAWN')]\n",
    "\n",
    "# weblinks for previewing / checking results\n",
    "weblinks_df = fr_eng_correlation_df.copy()\n",
    "weblinks_df = weblinks_df[['pub_number', 'nom', 'name', 'url_fr', 'url_en', 'file_url_fr', 'file_url_en']]\n",
    "\n",
    "# data to be used for language classifier\n",
    "lang_df = fr_eng_correlation_df.copy()\n",
    "lang_df = lang_df[(lang_df.filename_fr.isin(parsed_files_with_hq_ocr)) & (lang_df.filename_en.isin(parsed_files_with_hq_ocr)) & (lang_df.filename_fr != lang_df.filename_en)]\n"
   ],
   "id": "83675b54298a8486",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# helper functions",
   "id": "ca8dc9ee3a7f7d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:42:52.276545Z",
     "start_time": "2025-01-30T13:42:52.267865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preview_publication(pub_number):\n",
    "    if type(pub_number) is pd.DataFrame and pub_number.shape[0] == 1:\n",
    "        try:\n",
    "            pub_number = pub_number['pub_number'].values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    elif type(pub_number) is pd.Series:\n",
    "        try:\n",
    "            pub_number = pub_number.values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        output_df = weblinks_df[weblinks_df.pub_number == pub_number].T\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_filepaths(row, min_year=2023):\n",
    "    fr_filename, en_filename = row['filename_fr'] + '.json', row['filename_en'] + '.json'\n",
    "    file_folders = [os.path.join('..', 'ParsedPublications', str(year)) for year in range(min_year, 2024 + 1)]\n",
    "    \n",
    "    try:\n",
    "        fr_path, en_path = ([os.path.join(folder, fr_filename) for folder in file_folders if os.path.exists(os.path.join(folder, fr_filename))][0], \n",
    "                            [os.path.join(folder, en_filename) for folder in file_folders if os.path.exists(os.path.join(folder, en_filename))][0])\n",
    "    except IndexError:\n",
    "        return None, None\n",
    "    \n",
    "    return fr_path, en_path\n"
   ],
   "id": "93d13f552b93b1a0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:04.334549Z",
     "start_time": "2025-01-30T13:42:52.292931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make lists of all French words and all English words\n",
    "\n",
    "valid_word_regex = re.compile(r'^[a-zA-ZÀ-ÿ]+$')\n",
    "french_word_list = []\n",
    "english_word_list = []\n",
    "exclude_words_with_less_than_n = 10\n",
    "\n",
    "# clean headers and appendices\n",
    "references_fr = r'RÉFÉRENCES CITÉES'.lower()\n",
    "references_en = r'REFERENCES CITED'.lower()\n",
    "\n",
    "\n",
    "for i, row in lang_df.iterrows():\n",
    "    fr_path, en_path = get_filepaths(row)\n",
    "    \n",
    "    with open(fr_path, 'r', encoding='utf-8') as file:\n",
    "        fr_text = json.load(file).get('text', '').lower()\n",
    "            \n",
    "        parts = re.split(references_fr, fr_text, flags=re.IGNORECASE)\n",
    "        if 2 < len(parts) < 5:  # if 2 or 3 occurences of references text, take the second part (to get the main body text)\n",
    "            fr_text = parts[1]\n",
    "        \n",
    "        french_word_list.extend(word for word in fr_text.split() if valid_word_regex.match(word))\n",
    "    \n",
    "    with open(en_path, 'r', encoding='utf-8') as file:\n",
    "        en_text = json.load(file).get('text', '').lower()\n",
    "            \n",
    "        parts = re.split(references_en, en_text, flags=re.IGNORECASE)\n",
    "        if 2 < len(parts) < 5:  # if 2 or 3 occurences of references text, take the second part (to get the main body text)\n",
    "            en_text = parts[1]\n",
    "        \n",
    "        english_word_list.extend(word for word in en_text.split() if valid_word_regex.match(word))\n",
    "        \n",
    "# For testing\n",
    "french_word_counts = Counter(french_word_list)\n",
    "french_word_counts_expanded = []\n",
    "for word, count in french_word_counts.items():\n",
    "    for _ in range(count):\n",
    "        french_word_counts_expanded.append((word, count))\n",
    "        \n",
    "english_word_counts = Counter(english_word_list)\n",
    "english_word_counts_expanded = []\n",
    "for word, count in english_word_counts.items():\n",
    "    for _ in range(count):\n",
    "        english_word_counts_expanded.append((word, count))\n",
    "\n",
    "full_french_word_list = french_word_list.copy()\n",
    "full_english_word_list = english_word_list.copy()\n",
    "\n",
    "# Remove words with less than 10 occurrences\n",
    "french_word_list = [word for word, count in french_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "english_word_list = [word for word, count in english_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "\n",
    "# Convert to sets for further processing\n",
    "french_words = set(french_word_list)\n",
    "english_words = set(english_word_list)\n",
    "\n",
    "# Remove overlapping words\n",
    "overlapping_words = english_words & french_words\n",
    "english_words.difference_update(overlapping_words)\n",
    "french_words.difference_update(overlapping_words)\n",
    "\n",
    "# Remove numeric-only words\n",
    "english_words.difference_update({w for w in english_words if w.isnumeric()})\n",
    "french_words.difference_update({w for w in french_words if w.isnumeric()})\n"
   ],
   "id": "713c097efeb1dc20",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:04.421867Z",
     "start_time": "2025-01-30T13:43:04.414109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper functions for word lists\n",
    "\n",
    "def test_wordlists(text_block, english_words, french_words):\n",
    "    en_count = sum(1 for word in text_block.split() if word in english_words)\n",
    "    fr_count = sum(1 for word in text_block.split() if word in french_words)\n",
    "    \n",
    "    print('english words:', list(word for word in text_block.split() if word in english_words))\n",
    "    print('french words:', list(word for word in text_block.split() if word in french_words))\n",
    "    print(f'{en_count=}, {fr_count=}')    \n",
    "    \n",
    "def most_common_word_info(counter_obj, n=10):  \n",
    "    length = counter_obj.total()\n",
    "    c_v = 0\n",
    "    for k, v in counter_obj.most_common(n):\n",
    "        rng = f'({(100 * c_v) / length:.0f}%'\n",
    "        c_v += v\n",
    "        rng += f'-{(100 * c_v) / length:.0f}%)'\n",
    "        print(f'{k:<20}{v:>8}{(100 * v) / length:>8.0f}%{rng:>15}')\n",
    "\n",
    "def nth_percentile(p, counter_obj, greater_than=True):\n",
    "    sorted_list = sorted(counter_obj.items(), key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return sorted_list[index]\n",
    "\n",
    "def nth_percentile_weighted(p, counter_expanded, greater_than=True):\n",
    "    sorted_list = sorted(counter_expanded, key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return sorted_list[index]\n",
    "\n",
    "def count_nth_percentile(p, counter_obj, greater_than=True):\n",
    "    sorted_list = sorted(counter_obj.items(), key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    return len(sorted_list[index:]) if greater_than else len(sorted_list[-max(index, 1):])\n",
    "\n",
    "def count_nth_percentile_weighted(p, counter_expanded, greater_than=True):\n",
    "    sorted_list = sorted(counter_expanded, key=lambda x: x[1], reverse=greater_than)\n",
    "    index = max(min(len(sorted_list) - 1, int(len(sorted_list) * p)), 0)\n",
    "    \n",
    "    count_if_gte = sorted_list[len(sorted_list) - index][1]\n",
    "    count_if_lte = sorted_list[index][1]\n",
    "        \n",
    "    gte = {x for x in counter_expanded if x[1] >= count_if_gte}\n",
    "    lte = {x for x in counter_expanded if x[1] <= count_if_lte}\n",
    "    \n",
    "    return len(gte) if greater_than else len(lte)\n",
    "\n",
    "def examples_at_word_count(n, counter_obj, n_samples=5, tolerance=0.1):\n",
    "    lower, upper = int((1 - 0.1) * n), int((1 + 0.1) * n)\n",
    "    all_examples = [k for k, v in counter_obj.items() if lower <= v <= upper]\n",
    "    \n",
    "    return random.sample(all_examples, min(n_samples, len(all_examples)))\n"
   ],
   "id": "83648ca204fa96b0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:04.472027Z",
     "start_time": "2025-01-30T13:43:04.461269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_wordlists('Total mortalities at age, based on survey data, are presented in Table', english_words, french_words)\n",
    "print()\n",
    "most_common_word_info(english_word_counts, 4)\n",
    "print()\n",
    "most_common_word_info(french_word_counts, 4)"
   ],
   "id": "4b842b447ae4f093",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english words: ['mortalities', 'presented']\n",
      "french words: []\n",
      "en_count=2, fr_count=0\n",
      "\n",
      "the                   168894       7%        (0%-7%)\n",
      "of                     94029       4%       (7%-11%)\n",
      "and                    93778       4%      (11%-15%)\n",
      "in                     72480       3%      (15%-19%)\n",
      "\n",
      "de                    237366       8%        (0%-8%)\n",
      "la                    140133       5%       (8%-13%)\n",
      "et                    100646       4%      (13%-17%)\n",
      "les                   100095       4%      (17%-21%)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:05.351647Z",
     "start_time": "2025-01-30T13:43:04.552232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.1, 0.5, 0.95]:\n",
    "    print(p, nth_percentile(p, english_word_counts), nth_percentile(p, english_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, english_word_counts_expanded), nth_percentile_weighted(p, english_word_counts_expanded, False))\n",
    "\n",
    "print()\n",
    "\n",
    "for n in [1, 10, 100, 1000, 10000]:\n",
    "    print(n, examples_at_word_count(n, english_word_counts))"
   ],
   "id": "57387b89a4c41acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 ('endemism', 87) ('terebratulina', 1)\n",
      "0.1 weighted ('of', 94029) ('adolescent', 99)\n",
      "0.5 ('contradiction', 3) ('hshlf', 3)\n",
      "0.5 weighted ('landings', 2571) ('landings', 2571)\n",
      "0.95 ('biron', 1) ('linear', 238)\n",
      "0.95 weighted ('cobble', 34) ('the', 168894)\n",
      "\n",
      "1 ['dermochelys', 'redefinition', 'chrome', 'leaded', 'barite']\n",
      "10 ['psa', 'doubt', 'scientifique', 'ai', 'carroll']\n",
      "100 ['alteration', 'benchmarks', 'nations', 'accurately', 'recognized']\n",
      "1000 ['bc', 'decline', 'rivers', 'vessel', 'productivity']\n",
      "10000 ['not', 'data', 'at']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:44:30.130539Z",
     "start_time": "2025-01-30T13:44:29.117404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.001, 0.01, .99, .999]:\n",
    "    print(p, nth_percentile(p, english_word_counts), nth_percentile(p, english_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, english_word_counts_expanded), nth_percentile_weighted(p, english_word_counts_expanded, False))"
   ],
   "id": "8ea7020297359e47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ('an', 7322) ('skipper', 1)\n",
      "0.001 weighted ('the', 168894) ('steedman', 1)\n",
      "0.01 ('indicators', 1239) ('firefighting', 1)\n",
      "0.01 weighted ('the', 168894) ('myxine', 4)\n",
      "0.99 ('cognition', 1) ('indicators', 1239)\n",
      "0.99 weighted ('visualizing', 4) ('the', 168894)\n",
      "0.999 ('jacking', 1) ('an', 7322)\n",
      "0.999 weighted ('bloch', 1) ('the', 168894)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:06.366271Z",
     "start_time": "2025-01-30T13:43:05.391840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.1, 0.5, 0.95]:\n",
    "    print(p, nth_percentile(p, french_word_counts), nth_percentile(p, french_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, french_word_counts_expanded), nth_percentile_weighted(p, french_word_counts_expanded, False))\n",
    "\n",
    "print()\n",
    "\n",
    "for n in [1, 10, 100, 1000, 10000]:\n",
    "    print(n, examples_at_word_count(n, french_word_counts))"
   ],
   "id": "71663313fedcb0ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 ('appelée', 64) ('héberge', 1)\n",
      "0.1 weighted ('la', 140133) ('estivale', 96)\n",
      "0.5 ('cubiques', 3) ('résistent', 3)\n",
      "0.5 weighted ('ne', 5866) ('ne', 5866)\n",
      "0.95 ('interagency', 1) ('conseil', 172)\n",
      "0.95 weighted ('simard', 32) ('de', 237366)\n",
      "\n",
      "1 ['drauch', 'dependent', 'entraveraient', 'autoroutes', 'cristina']\n",
      "10 ['pétrolier', 'sépare', 'retrouvées', 'bocaccios', 'kumar']\n",
      "100 ['incluant', 'océaniques', 'acceptent', 'considère', 'considération']\n",
      "1000 ['indice', 'tac', 'inférieure', 'capacité', 'cas']\n",
      "10000 ['ou', 'avec', 'pas', 'aux']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:45:08.349808Z",
     "start_time": "2025-01-30T13:45:07.084853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in [0.001, 0.01, .99, .999]:\n",
    "    print(p, nth_percentile(p, french_word_counts), nth_percentile(p, french_word_counts, False))\n",
    "    print(p, 'weighted', nth_percentile_weighted(p, french_word_counts_expanded), nth_percentile_weighted(p, french_word_counts_expanded, False))"
   ],
   "id": "af94be3f9391a85c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ('il', 8012) ('convoquer', 1)\n",
      "0.001 weighted ('de', 237366) ('plivelic', 1)\n",
      "0.01 ('différentes', 954) ('opérateurs', 1)\n",
      "0.01 weighted ('de', 237366) ('synonyme', 4)\n",
      "0.99 ('fréquentations', 1) ('différentes', 954)\n",
      "0.99 weighted ('rein', 4) ('de', 237366)\n",
      "0.999 ('retirez', 1) ('il', 8012)\n",
      "0.999 weighted ('strub', 1) ('de', 237366)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:07:33.985122Z",
     "start_time": "2025-01-30T14:07:33.970793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(text, references_pattern):\n",
    "    valid_word_regex = re.compile(r'^[a-zA-ZÀ-ÿ]+$')\n",
    "    min_length, max_length = 5, 20\n",
    "    \n",
    "    document_parts = re.split(references_pattern, text, flags=re.IGNORECASE)\n",
    "    if 2 < len(document_parts) < 5:\n",
    "        text = document_parts[1]\n",
    "    word_list = [word for word in text.split() if valid_word_regex.match(word)]\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^a-zA-ZÀ-ÿ.\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    sentence_list = [\n",
    "        sentence.strip() for sentence in cleaned_text.split('.')\n",
    "        if min_length <= len(sentence.split()) <= max_length\n",
    "        and all(valid_word_regex.match(word) for word in sentence.split())\n",
    "    ]\n",
    "    \n",
    "    return word_list, sentence_list\n",
    "\n",
    "\n",
    "def generate_word_lists(n):\n",
    "    french_word_list = []\n",
    "    english_word_list = []\n",
    "    exclude_words_with_less_than_n = n\n",
    "    \n",
    "    # example sentences\n",
    "    french_example_sentences = []\n",
    "    english_example_sentences = []\n",
    "    \n",
    "    references_fr = r'RÉFÉRENCES CITÉES'.lower()\n",
    "    references_en = r'REFERENCES CITED'.lower()\n",
    "        \n",
    "    for i, row in lang_df.iterrows():\n",
    "        fr_path, en_path = get_filepaths(row)\n",
    "        \n",
    "        with open(fr_path, 'r', encoding='utf-8') as file:\n",
    "            fr_text = json.load(file).get('text', '').lower()\n",
    "            word_list, sentence_list = process_text(fr_text, references_fr)\n",
    "            french_word_list.extend(word_list)\n",
    "            french_example_sentences.extend(sentence_list)\n",
    "        \n",
    "        with open(en_path, 'r', encoding='utf-8') as file:\n",
    "            en_text = json.load(file).get('text', '').lower()\n",
    "            word_list, sentence_list = process_text(en_text, references_en)\n",
    "            english_word_list.extend(word_list)\n",
    "            english_example_sentences.extend(sentence_list)\n",
    "            \n",
    "    # Remove words with less than 10 occurrences\n",
    "    french_word_list = [word for word, count in french_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "    english_word_list = [word for word, count in english_word_counts.items() if count >= exclude_words_with_less_than_n]\n",
    "    \n",
    "    # Convert to sets for further processing\n",
    "    french_words = set(french_word_list)\n",
    "    english_words = set(english_word_list)\n",
    "    \n",
    "    # Remove overlapping words\n",
    "    overlapping_words = english_words & french_words\n",
    "    english_words.difference_update(overlapping_words)\n",
    "    french_words.difference_update(overlapping_words)\n",
    "    \n",
    "    # Remove numeric-only words\n",
    "    english_words.difference_update({w for w in english_words if w.isnumeric()})\n",
    "    french_words.difference_update({w for w in french_words if w.isnumeric()})\n",
    "    \n",
    "    return english_words, french_words, french_example_sentences, english_example_sentences\n"
   ],
   "id": "d61afdf21f22c367",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:33:36.132653Z",
     "start_time": "2025-01-30T16:33:23.941974Z"
    }
   },
   "cell_type": "code",
   "source": "_, _, french_example_sentences, english_example_sentences = generate_word_lists(0)",
   "id": "9448a2ce4274425c",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:02:56.664124Z",
     "start_time": "2025-01-30T14:02:56.646148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n in [0, 1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    print(n, sum([1 for x in french_word_counts.values() if x > n]), sum([1 for x in english_word_counts.values() if x > n]))"
   ],
   "id": "da17c0a6bc8a47aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31713 25194\n",
      "1 21977 17713\n",
      "5 12428 10323\n",
      "10 9087 7741\n",
      "20 6304 5591\n",
      "50 3716 3492\n",
      "100 2327 2301\n",
      "200 1397 1436\n",
      "500 618 657\n",
      "1000 298 319\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:07:55.172471Z",
     "start_time": "2025-01-30T14:07:55.157154Z"
    }
   },
   "cell_type": "code",
   "source": "random.sample(french_example_sentences, 3)",
   "id": "40174dc35ec2fc35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['résultats de lestimation de la température au fond car pour chaque année',\n",
       " 'ces unités seront utilisées pour aider à atteindre le critère de représentativité pour la conception du réseau damp',\n",
       " 'ajustements du modèle et étalonnage fondé sur la longueur sélectionné pour triglops murrayi']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# n_trials = 10\n",
    "# n_to_exclude = [0, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "# \n",
    "# french_example_sentences_n = random.sample(french_example_sentences, n_trials)\n",
    "# english_example_sentences_n = random.sample(english_example_sentences, n_trials)\n",
    "# results = []\n",
    "# \n",
    "# for n in n_to_exclude:\n",
    "#     print(f'Processing {n}')\n",
    "#     english_words, french_words, french_example_sentences, english_example_sentences = generate_word_lists(n)\n",
    "#     \n",
    "#     for sentence in french_example_sentences_n:\n",
    "#         fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "#         en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "#         results.append((n, 'fr', fr_count, en_count))\n",
    "#     \n",
    "#     for sentence in english_example_sentences_n:\n",
    "#         fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "#         en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "#         results.append((n, 'en', fr_count, en_count))\n",
    "#     \n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.columns = ['n_excluded', 'language', 'fr_count', 'en_count']\n",
    "# \n",
    "# results_df['total_count'] = results_df['fr_count'] + results_df['en_count']\n",
    "# \n",
    "# valid_mask = results_df['total_count'] > 0\n",
    "# \n",
    "# results_df['p_correct'] = np.where(\n",
    "#     (results_df['language'] == 'fr') & valid_mask,\n",
    "#     results_df['fr_count'] / results_df['total_count'],\n",
    "#     np.where(\n",
    "#         (results_df['language'] == 'en') & valid_mask,\n",
    "#         results_df['en_count'] / results_df['total_count'],\n",
    "#         0\n",
    "#     )\n",
    "# )\n",
    "# \n",
    "# results_df['p_wrong'] = np.where(\n",
    "#     (results_df['language'] == 'fr') & valid_mask,\n",
    "#     results_df['en_count'] / results_df['total_count'],\n",
    "#     np.where(\n",
    "#         (results_df['language'] == 'en') & valid_mask,\n",
    "#         results_df['fr_count'] / results_df['total_count'],\n",
    "#         0\n",
    "#     )\n",
    "# )"
   ],
   "id": "8a3128b05c90d8ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T15:16:47.012320Z",
     "start_time": "2025-01-30T15:16:46.996067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def display_results(n, show_languages=False):\n",
    "#     print('TOTAL')\n",
    "#     display(results_df.loc[results_df.n_excluded == n, ['total_count', 'p_correct', 'p_wrong']].describe([0.025, .975]).T[['2.5%', 'mean', '97.5%']])\n",
    "#     if show_languages:\n",
    "#         print('FRENCH')\n",
    "#         display(results_df.loc[(results_df.n_excluded == n) & (results_df.language == 'fr'), ['total_count', 'p_correct', 'p_wrong']].describe([0.025, .975]).T[['2.5%', 'mean', '97.5%']])\n",
    "#         print('ENGLISH')\n",
    "#         display(results_df.loc[(results_df.n_excluded == n) & (results_df.language == 'en'), ['total_count', 'p_correct', 'p_wrong']].describe([0.025, .975]).T[['2.5%', 'mean', '97.5%']])\n",
    "#     print('\\n')\n",
    "# \n",
    "# for n in n_to_exclude:\n",
    "#     print(f'\\nWords with below {n} counts excluded\\n')\n",
    "#     display_results(n)"
   ],
   "id": "2c63f45bf5eb6c0f",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce82604012f617b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:07:51.677401Z",
     "start_time": "2025-01-30T16:06:56.146258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_trials = 100\n",
    "n_to_exclude = [0, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "french_example_sentences_n = random.sample(french_example_sentences, n_trials)\n",
    "english_example_sentences_n = random.sample(english_example_sentences, n_trials)\n",
    "results = []\n",
    "\n",
    "for n in n_to_exclude:\n",
    "    print(f'Processing {n}')\n",
    "    english_words, french_words, french_example_sentences, english_example_sentences = generate_word_lists(n)\n",
    "    \n",
    "    for sentence in french_example_sentences_n:\n",
    "        fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "        en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "        results.append((n, 'fr', fr_count, en_count))\n",
    "    \n",
    "    for sentence in english_example_sentences_n:\n",
    "        fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "        en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "        results.append((n, 'en', fr_count, en_count))\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['n_excluded', 'language', 'fr_count', 'en_count']\n",
    "\n",
    "results_df['total_count'] = results_df['fr_count'] + results_df['en_count']\n",
    "\n",
    "valid_mask = results_df['total_count'] > 0\n",
    "\n",
    "results_df['correct_count'] = np.where(\n",
    "    (results_df['language'] == 'fr') & valid_mask,\n",
    "    results_df['fr_count'],\n",
    "    np.where(\n",
    "        (results_df['language'] == 'en') & valid_mask,\n",
    "        results_df['en_count'],\n",
    "        0\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df['wrong_count'] = np.where(\n",
    "    (results_df['language'] == 'fr') & valid_mask,\n",
    "    results_df['en_count'],\n",
    "    np.where(\n",
    "        (results_df['language'] == 'en') & valid_mask,\n",
    "        results_df['fr_count'],\n",
    "        0\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df['is_correct'] = results_df['correct_count'] > results_df['wrong_count']\n"
   ],
   "id": "7120e7465f02ff17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Processing 1\n",
      "Processing 5\n",
      "Processing 10\n",
      "Processing 20\n",
      "Processing 50\n",
      "Processing 100\n",
      "Processing 200\n",
      "Processing 500\n",
      "Processing 1000\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:07:51.687201Z",
     "start_time": "2025-01-30T16:07:51.681503Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.head()",
   "id": "5104c37d757221d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   n_excluded language  fr_count  en_count  total_count  correct_count  \\\n",
       "0           0       fr         3         0            3              3   \n",
       "1           0       fr         7         0            7              7   \n",
       "2           0       fr         3         0            3              3   \n",
       "3           0       fr         4         0            4              4   \n",
       "4           0       fr         3         0            3              3   \n",
       "\n",
       "   wrong_count  is_correct  \n",
       "0            0        True  \n",
       "1            0        True  \n",
       "2            0        True  \n",
       "3            0        True  \n",
       "4            0        True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_excluded</th>\n",
       "      <th>language</th>\n",
       "      <th>fr_count</th>\n",
       "      <th>en_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>correct_count</th>\n",
       "      <th>wrong_count</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:19:57.694757Z",
     "start_time": "2025-01-30T16:19:57.667869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define FP & FN for each language\n",
    "results_df['fr_false_positive'] = (results_df['language'] == 'en') & (results_df['is_correct'] == False)\n",
    "results_df['fr_false_negative'] = (results_df['language'] == 'fr') & (results_df['is_correct'] == False)\n",
    "\n",
    "results_df['en_false_positive'] = (results_df['language'] == 'fr') & (results_df['is_correct'] == False)\n",
    "results_df['en_false_negative'] = (results_df['language'] == 'en') & (results_df['is_correct'] == False)\n",
    "\n",
    "# Compute separate aggregations\n",
    "grouped_df = results_df.groupby('n_excluded').agg(\n",
    "    total_count=('total_count', 'sum'),\n",
    "    \n",
    "    # Correct and incorrect classifications\n",
    "    correct_count=('is_correct', 'sum'),\n",
    "    wrong_count=('is_correct', lambda x: (~x).sum()),  \n",
    "\n",
    "    # False Positives & False Negatives for each language\n",
    "    fr_false_positive=('fr_false_positive', 'sum'),\n",
    "    fr_false_negative=('fr_false_negative', 'sum'),\n",
    "    en_false_positive=('en_false_positive', 'sum'),\n",
    "    en_false_negative=('en_false_negative', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Avoid division by zero\n",
    "valid_mask = grouped_df['total_count'] > 0\n",
    "\n",
    "# Accuracy (same for both languages)\n",
    "grouped_df['accuracy'] = np.where(valid_mask, grouped_df['correct_count'] / grouped_df['total_count'], 0)\n",
    "\n",
    "# Precision & Recall for French\n",
    "fr_precision_mask = (grouped_df['correct_count'] + grouped_df['fr_false_positive']) > 0\n",
    "grouped_df['fr_precision'] = np.where(\n",
    "    fr_precision_mask, \n",
    "    grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['fr_false_positive']), \n",
    "    0\n",
    ")\n",
    "\n",
    "fr_recall_mask = (grouped_df['correct_count'] + grouped_df['fr_false_negative']) > 0\n",
    "grouped_df['fr_recall'] = np.where(\n",
    "    fr_recall_mask, \n",
    "    grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['fr_false_negative']), \n",
    "    0\n",
    ")\n",
    "\n",
    "# Precision & Recall for English\n",
    "en_precision_mask = (grouped_df['correct_count'] + grouped_df['en_false_positive']) > 0\n",
    "grouped_df['en_precision'] = np.where(\n",
    "    en_precision_mask, \n",
    "    grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['en_false_positive']), \n",
    "    0\n",
    ")\n",
    "\n",
    "en_recall_mask = (grouped_df['correct_count'] + grouped_df['en_false_negative']) > 0\n",
    "grouped_df['en_recall'] = np.where(\n",
    "    en_recall_mask, \n",
    "    grouped_df['correct_count'] / (grouped_df['correct_count'] + grouped_df['en_false_negative']), \n",
    "    0\n",
    ")\n",
    "\n",
    "# Compute F1-score separately\n",
    "grouped_df['fr_f1_score'] = np.where(\n",
    "    (grouped_df['fr_precision'] + grouped_df['fr_recall']) > 0,\n",
    "    2 * (grouped_df['fr_precision'] * grouped_df['fr_recall']) / (grouped_df['fr_precision'] + grouped_df['fr_recall']),\n",
    "    0\n",
    ")\n",
    "\n",
    "grouped_df['en_f1_score'] = np.where(\n",
    "    (grouped_df['en_precision'] + grouped_df['en_recall']) > 0,\n",
    "    2 * (grouped_df['en_precision'] * grouped_df['en_recall']) / (grouped_df['en_precision'] + grouped_df['en_recall']),\n",
    "    0\n",
    ")"
   ],
   "id": "ce8ea2c057ffe7fc",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:23:43.195455Z",
     "start_time": "2025-01-30T16:23:43.174488Z"
    }
   },
   "cell_type": "code",
   "source": "grouped_df.set_index('n_excluded').T",
   "id": "cb44d9b835742b3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_excluded          0      1      5      10      20      50      100     200   \\\n",
       "total_count       528.00 528.00 835.00 906.00 1057.00 1152.00 1229.00 1278.00   \n",
       "correct_count     155.00 155.00 171.00 174.00  178.00  183.00  183.00  180.00   \n",
       "wrong_count        45.00  45.00  29.00  26.00   22.00   17.00   17.00   20.00   \n",
       "fr_false_positive  29.00  29.00  15.00  13.00   10.00    4.00    4.00    6.00   \n",
       "fr_false_negative  16.00  16.00  14.00  13.00   12.00   13.00   13.00   14.00   \n",
       "en_false_positive  16.00  16.00  14.00  13.00   12.00   13.00   13.00   14.00   \n",
       "en_false_negative  29.00  29.00  15.00  13.00   10.00    4.00    4.00    6.00   \n",
       "accuracy            0.29   0.29   0.20   0.19    0.17    0.16    0.15    0.14   \n",
       "fr_precision        0.84   0.84   0.92   0.93    0.95    0.98    0.98    0.97   \n",
       "fr_recall           0.91   0.91   0.92   0.93    0.94    0.93    0.93    0.93   \n",
       "en_precision        0.91   0.91   0.92   0.93    0.94    0.93    0.93    0.93   \n",
       "en_recall           0.84   0.84   0.92   0.93    0.95    0.98    0.98    0.97   \n",
       "fr_f1_score         0.87   0.87   0.92   0.93    0.94    0.96    0.96    0.95   \n",
       "en_f1_score         0.87   0.87   0.92   0.93    0.94    0.96    0.96    0.95   \n",
       "\n",
       "n_excluded           500     1000  \n",
       "total_count       1223.00 1210.00  \n",
       "correct_count      181.00  183.00  \n",
       "wrong_count         19.00   17.00  \n",
       "fr_false_positive    5.00    6.00  \n",
       "fr_false_negative   14.00   11.00  \n",
       "en_false_positive   14.00   11.00  \n",
       "en_false_negative    5.00    6.00  \n",
       "accuracy             0.15    0.15  \n",
       "fr_precision         0.97    0.97  \n",
       "fr_recall            0.93    0.94  \n",
       "en_precision         0.93    0.94  \n",
       "en_recall            0.97    0.97  \n",
       "fr_f1_score          0.95    0.96  \n",
       "en_f1_score          0.95    0.96  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_excluded</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>500</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_count</th>\n",
       "      <td>528.00</td>\n",
       "      <td>528.00</td>\n",
       "      <td>835.00</td>\n",
       "      <td>906.00</td>\n",
       "      <td>1057.00</td>\n",
       "      <td>1152.00</td>\n",
       "      <td>1229.00</td>\n",
       "      <td>1278.00</td>\n",
       "      <td>1223.00</td>\n",
       "      <td>1210.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_count</th>\n",
       "      <td>155.00</td>\n",
       "      <td>155.00</td>\n",
       "      <td>171.00</td>\n",
       "      <td>174.00</td>\n",
       "      <td>178.00</td>\n",
       "      <td>183.00</td>\n",
       "      <td>183.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>181.00</td>\n",
       "      <td>183.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong_count</th>\n",
       "      <td>45.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_positive</th>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_false_negative</th>\n",
       "      <td>16.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_positive</th>\n",
       "      <td>16.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_false_negative</th>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_precision</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_recall</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_precision</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_recall</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_f1_score</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en_f1_score</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:25:00.938870Z",
     "start_time": "2025-01-30T16:25:00.924750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grouped_df.set_index('n_excluded').drop(['total_count', 'fr_false_positive',\n",
    "       'fr_false_negative', 'en_false_positive', 'en_false_negative'], axis=1)"
   ],
   "id": "269c750da1846626",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            correct_count  wrong_count  accuracy  fr_precision  fr_recall  \\\n",
       "n_excluded                                                                  \n",
       "0                     155           45      0.29          0.84       0.91   \n",
       "1                     155           45      0.29          0.84       0.91   \n",
       "5                     171           29      0.20          0.92       0.92   \n",
       "10                    174           26      0.19          0.93       0.93   \n",
       "20                    178           22      0.17          0.95       0.94   \n",
       "50                    183           17      0.16          0.98       0.93   \n",
       "100                   183           17      0.15          0.98       0.93   \n",
       "200                   180           20      0.14          0.97       0.93   \n",
       "500                   181           19      0.15          0.97       0.93   \n",
       "1000                  183           17      0.15          0.97       0.94   \n",
       "\n",
       "            en_precision  en_recall  fr_f1_score  en_f1_score  \n",
       "n_excluded                                                     \n",
       "0                   0.91       0.84         0.87         0.87  \n",
       "1                   0.91       0.84         0.87         0.87  \n",
       "5                   0.92       0.92         0.92         0.92  \n",
       "10                  0.93       0.93         0.93         0.93  \n",
       "20                  0.94       0.95         0.94         0.94  \n",
       "50                  0.93       0.98         0.96         0.96  \n",
       "100                 0.93       0.98         0.96         0.96  \n",
       "200                 0.93       0.97         0.95         0.95  \n",
       "500                 0.93       0.97         0.95         0.95  \n",
       "1000                0.94       0.97         0.96         0.96  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_count</th>\n",
       "      <th>wrong_count</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fr_precision</th>\n",
       "      <th>fr_recall</th>\n",
       "      <th>en_precision</th>\n",
       "      <th>en_recall</th>\n",
       "      <th>fr_f1_score</th>\n",
       "      <th>en_f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_excluded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155</td>\n",
       "      <td>45</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>45</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>171</td>\n",
       "      <td>29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>174</td>\n",
       "      <td>26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>178</td>\n",
       "      <td>22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>183</td>\n",
       "      <td>17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>183</td>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>181</td>\n",
       "      <td>19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>183</td>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:24:03.797798Z",
     "start_time": "2025-01-30T16:24:03.792405Z"
    }
   },
   "cell_type": "code",
   "source": "best_by_feature = [50, 100, 1000]",
   "id": "7f4133dc40d301f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_count', 'correct_count', 'wrong_count', 'fr_false_positive',\n",
       "       'fr_false_negative', 'en_false_positive', 'en_false_negative',\n",
       "       'accuracy', 'fr_precision', 'fr_recall', 'en_precision', 'en_recall',\n",
       "       'fr_f1_score', 'en_f1_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T15:38:37.279386800Z",
     "start_time": "2025-01-30T15:29:42.504383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_df.groupby('n_excluded').agg(\n",
    "    mean_correct=('correct_count', 'mean'),\n",
    "    mean_wrong=('wrong_count', 'mean'),\n",
    "    mean_accuracy=('accuracy', 'mean'),\n",
    "    mean_precision=('precision', 'mean'),\n",
    "    mean_recall=('recall', 'mean'),\n",
    "    mean_f1_score=('f1_score', 'mean'),\n",
    ").reset_index()"
   ],
   "id": "9807555973ca2908",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   n_excluded  mean_correct  mean_wrong  mean_accuracy  mean_precision  \\\n",
       "0           0          2.83        0.01           0.81            0.81   \n",
       "1           1          2.83        0.01           0.81            0.81   \n",
       "2           5          4.42        0.08           0.90            0.90   \n",
       "3          10          4.98        0.12           0.92            0.92   \n",
       "4          20          5.64        0.19           0.91            0.91   \n",
       "5          50          6.07        0.29           0.92            0.92   \n",
       "6         100          6.38        0.39           0.93            0.93   \n",
       "7         200          6.54        0.35           0.93            0.93   \n",
       "8         500          6.13        0.39           0.93            0.93   \n",
       "9        1000          5.97        0.33           0.92            0.92   \n",
       "\n",
       "   mean_recall  mean_f1_score  \n",
       "0         0.81           0.81  \n",
       "1         0.81           0.81  \n",
       "2         0.90           0.90  \n",
       "3         0.92           0.92  \n",
       "4         0.91           0.91  \n",
       "5         0.92           0.92  \n",
       "6         0.93           0.93  \n",
       "7         0.93           0.93  \n",
       "8         0.93           0.93  \n",
       "9         0.92           0.92  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_excluded</th>\n",
       "      <th>mean_correct</th>\n",
       "      <th>mean_wrong</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>mean_precision</th>\n",
       "      <th>mean_recall</th>\n",
       "      <th>mean_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T15:38:37.279386800Z",
     "start_time": "2025-01-30T15:32:23.941401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backup old results_df\n",
    "results_df_BACKUP = results_df.copy() "
   ],
   "id": "36cb01715f31204d",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T15:39:36.946538Z",
     "start_time": "2025-01-30T15:39:28.706422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# repeated with tweaked hyperparams\n",
    "\n",
    "n_trials = 1000\n",
    "n_to_exclude = [x for x in range(50, 550, 50)] + [x for x in range(600, 1100, 100)] + [x for x in range(1200, 2200, 200)]\n",
    "\n",
    "french_example_sentences_n = random.sample(french_example_sentences, n_trials)\n",
    "english_example_sentences_n = random.sample(english_example_sentences, n_trials)\n",
    "results = []\n",
    "\n",
    "for n in n_to_exclude:\n",
    "    print(f'Processing {n}')\n",
    "    english_words, french_words, _, _ = generate_word_lists(n)\n",
    "    \n",
    "    for sentence in french_example_sentences_n:\n",
    "        fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "        en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "        results.append((n, 'fr', fr_count, en_count))\n",
    "    \n",
    "    for sentence in english_example_sentences_n:\n",
    "        fr_count = sum(1 for word in sentence.split() if word in french_words)\n",
    "        en_count = sum(1 for word in sentence.split() if word in english_words)\n",
    "        results.append((n, 'en', fr_count, en_count))\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['n_excluded', 'language', 'fr_count', 'en_count']\n",
    "\n",
    "results_df['total_count'] = results_df['fr_count'] + results_df['en_count']\n",
    "\n",
    "valid_mask = results_df['total_count'] > 0\n",
    "\n",
    "results_df['correct_count'] = np.where(\n",
    "    (results_df['language'] == 'fr') & valid_mask,\n",
    "    results_df['fr_count'],\n",
    "    np.where(\n",
    "        (results_df['language'] == 'en') & valid_mask,\n",
    "        results_df['en_count'],\n",
    "        0\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df['wrong_count'] = np.where(\n",
    "    (results_df['language'] == 'fr') & valid_mask,\n",
    "    results_df['en_count'],\n",
    "    np.where(\n",
    "        (results_df['language'] == 'en') & valid_mask,\n",
    "        results_df['fr_count'],\n",
    "        0\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df['is_correct'] = results_df['correct_count'] > results_df['wrong_count']"
   ],
   "id": "d160dc4f8f47c922",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[126], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m n_to_exclude:\n\u001B[1;32m---> 11\u001B[0m     english_words, french_words, french_example_sentences, english_example_sentences \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_word_lists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m french_example_sentences_n:\n\u001B[0;32m     14\u001B[0m         fr_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m sentence\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m french_words)\n",
      "Cell \u001B[1;32mIn[35], line 38\u001B[0m, in \u001B[0;36mgenerate_word_lists\u001B[1;34m(n)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(fr_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     37\u001B[0m     fr_text \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mlower()\n\u001B[1;32m---> 38\u001B[0m     word_list, sentence_list \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfr_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreferences_fr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     french_word_list\u001B[38;5;241m.\u001B[39mextend(word_list)\n\u001B[0;32m     40\u001B[0m     french_example_sentences\u001B[38;5;241m.\u001B[39mextend(sentence_list)\n",
      "Cell \u001B[1;32mIn[35], line 11\u001B[0m, in \u001B[0;36mprocess_text\u001B[1;34m(text, references_pattern)\u001B[0m\n\u001B[0;32m      8\u001B[0m word_list \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mif\u001B[39;00m valid_word_regex\u001B[38;5;241m.\u001B[39mmatch(word)]\n\u001B[0;32m     10\u001B[0m cleaned_text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^a-zA-ZÀ-ÿ.\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m---> 11\u001B[0m cleaned_text \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43ms+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcleaned_text\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m sentence_list \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     13\u001B[0m     sentence\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m cleaned_text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m min_length \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(sentence\u001B[38;5;241m.\u001B[39msplit()) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m max_length\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(valid_word_regex\u001B[38;5;241m.\u001B[39mmatch(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m sentence\u001B[38;5;241m.\u001B[39msplit())\n\u001B[0;32m     16\u001B[0m ]\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m word_list, sentence_list\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7ef88cd1369058c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b278789fc3d6193a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9bf32cd07687f4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1210b04ef632b07c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "724548838a486e3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3cf24683fd897b2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b096fb5d9b2dd699"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:43:06.853055700Z",
     "start_time": "2025-01-28T18:45:09.423940Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b92f2debd2a73216",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
