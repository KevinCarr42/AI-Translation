{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T12:52:36.600185Z",
     "start_time": "2025-02-26T12:52:14.419150Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from language_classifier.language_classifier import LanguageClassifier\n",
    "\n",
    "# formatting\n",
    "pd.set_option('display.float_format', '{:.1f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# import data",
   "id": "a9aae87a2e850e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:52:36.671511Z",
     "start_time": "2025-02-26T12:52:36.604726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# folders\n",
    "parsed_docs_folder = os.path.join(\"..\", \"ParsedPublications\")\n",
    "fr_eng_correlation_csv = \"fr_eng_correlation_data.csv\"\n",
    "\n",
    "fr_eng_correlation_df = pd.read_csv(fr_eng_correlation_csv)\n",
    "\n",
    "# weblinks for previewing / testing\n",
    "weblinks_df = fr_eng_correlation_df.copy()\n",
    "weblinks_df = weblinks_df[['pub_number', 'nom', 'name', 'url_fr', 'url_en', 'file_url_fr', 'file_url_en']]\n",
    "\n",
    "# simplified correlation table\n",
    "fr_eng_correlation_df = fr_eng_correlation_df[['pub_number', 'filename_fr', 'filename_en']]"
   ],
   "id": "83675b54298a8486",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# helper functions",
   "id": "ca8dc9ee3a7f7d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:52:37.268326Z",
     "start_time": "2025-02-26T12:52:37.253604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATA CLEANING FUNCTIONS\n",
    "\n",
    "def clean_text(text, skip_cleaning=False):\n",
    "    if not skip_cleaning:\n",
    "        allowed_chars = r\"[^a-zA-ZÀ-ÖØ-öø-ÿ0-9.,;:!?()'\\\"-]\"\n",
    "        text = re.sub(allowed_chars, ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_files_for_publication(pub_number, fr_eng_correlation_df):\n",
    "    row = fr_eng_correlation_df.loc[fr_eng_correlation_df['pub_number'] == pub_number]\n",
    "    if not row.empty:\n",
    "        filename_fr = row['filename_fr'].values[0]\n",
    "        filename_en = row['filename_en'].values[0]\n",
    "        return filename_fr, filename_en\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_json_file_link(parsed_docs_folder, pdf_filename):\n",
    "    if pdf_filename.endswith(\".pdf\"):\n",
    "        json_filename = pdf_filename + \".json\"\n",
    "        for root, _, files in os.walk(parsed_docs_folder):\n",
    "            if json_filename in files:\n",
    "                return os.path.join(root, json_filename)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text_from_single_file(json_file, target_language, clf, skip_cleaning=False):\n",
    "    min_block_length = 10\n",
    "    max_block_length = 500\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if 'text' not in data:\n",
    "        raise KeyError(f\"The key 'text' is missing in the JSON file: {json_file}\")\n",
    "    \n",
    "    full_text = clean_text(data['text'], skip_cleaning)\n",
    "    text_blocks = re.split(r'(?<![;,])[.?!]\\s|\\n\\n', full_text)\n",
    "    text = []\n",
    "\n",
    "    for block in text_blocks:\n",
    "        block = block.strip()\n",
    "        if len(block) < min_block_length or len(block) > max_block_length:\n",
    "            continue\n",
    "        \n",
    "        if clf.classify(block) == target_language:\n",
    "            text.append(block + '. ')      \n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def extract_both_languages_from_two_files(json_file_fr, json_file_en, clf, skip_cleaning=False):\n",
    "    return extract_text_from_single_file(json_file_fr, \"fr\", clf, skip_cleaning), extract_text_from_single_file(json_file_en, \"en\", clf, skip_cleaning)\n",
    "\n",
    "\n",
    "def extract_both_languages_from_single_file(json_file, clf, skip_cleaning=False):\n",
    "    min_block_length = 10\n",
    "    max_block_length = 500\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if 'text' not in data:\n",
    "        raise KeyError(f\"The key 'text' is missing in the JSON file: {json_file}\")\n",
    "    \n",
    "    full_text = clean_text(data['text'], skip_cleaning)\n",
    "    text_blocks = re.split(r'(?<![;,])[.?!]\\s|\\n\\n', full_text)\n",
    "    text_fr, text_en = [], []\n",
    "\n",
    "    for block in text_blocks:\n",
    "        block = block.strip()\n",
    "        if len(block) < min_block_length or len(block) > max_block_length:\n",
    "            continue\n",
    "            \n",
    "        if clf.classify(block) == \"fr\":\n",
    "            text_fr.append(block + '. ')   \n",
    "        elif clf.classify(block) == \"en\":\n",
    "            text_en.append(block + '. ')   \n",
    "\n",
    "    return \" \".join(text_fr), \" \".join(text_en)\n",
    "\n",
    "\n",
    "def create_sentences(text_fr, text_en):\n",
    "    sentences_fr = [x.strip() for x in re.split(r'(?<![;,])[.?!]\\s|\\n\\n', text_fr) if x != \"\"]\n",
    "    sentences_en = [x.strip() for x in re.split(r'(?<![;,])[.?!]\\s|\\n\\n', text_en) if x != \"\"]\n",
    "    \n",
    "    return sentences_fr, sentences_en\n",
    "\n",
    "\n",
    "def create_similarity_matrix(sentences_fr, sentences_en, sentence_encoder):\n",
    "    embeddings_fr = sentence_encoder.encode(sentences_fr, convert_to_tensor=True)\n",
    "    embeddings_en = sentence_encoder.encode(sentences_en, convert_to_tensor=True)\n",
    "\n",
    "    return util.pytorch_cos_sim(embeddings_fr, embeddings_en)\n",
    "\n",
    "\n",
    "# TODO: process_all_rows replaced with debug version (below) for this analysis\n",
    "\n",
    "\n",
    "def text_from_coordinates(aligned_pairs, sentences_fr, sentences_en, pub_number):\n",
    "    correlated_list = list()\n",
    "    for i, j in aligned_pairs:\n",
    "        correlated_list.append((pub_number, sentences_fr[i], sentences_en[j]))\n",
    "    \n",
    "    return correlated_list\n",
    "\n",
    "\n",
    "def correlate_and_clean_text(text_fr, text_en, pub_number, sentence_encoder):\n",
    "    sentences_fr, sentences_en = create_sentences(text_fr, text_en)\n",
    "    similarity_matrix = create_similarity_matrix(sentences_fr, sentences_en, sentence_encoder)\n",
    "    _, _, aligned_pairs = align_sentences(similarity_matrix)  # TODO: updated for EDA\n",
    "\n",
    "    return text_from_coordinates(aligned_pairs, sentences_fr, sentences_en, pub_number)\n",
    "\n",
    "\n",
    "# TODO: process_all_rows replaced with debug version (below) for this analysis\n",
    "\n",
    "\n",
    "# PREVIEW AND DEBUGGING\n",
    "\n",
    "def preview_publication(pub_number):\n",
    "    if type(pub_number) is pd.DataFrame and pub_number.shape[0] == 1:\n",
    "        try:\n",
    "            pub_number = pub_number['pub_number'].values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    elif type(pub_number) is pd.Series:\n",
    "        try:\n",
    "            pub_number = pub_number.values[0]\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        output_df = weblinks_df[weblinks_df.pub_number == pub_number].T\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "        \n",
    "    display(output_df)\n",
    "\n",
    "\n",
    "def preview_publication_by_row(row_n):\n",
    "    display(pd.DataFrame(matched_df.iloc[row_n]))\n",
    "    preview_publication(matched_df.iloc[row_n].pub_number)\n",
    "    \n",
    "    \n",
    "def plot_similarity_heatmap(similarity_matrix, figsize=(10, 10), low_similarity=0.6, high_similarity=0.8, matched_pairs=None, cmap=None, norm=None, square=True):\n",
    "    fig, ax = plt.subplots(figsize=figsize) \n",
    "    \n",
    "    if cmap:\n",
    "        if norm:\n",
    "            sns.heatmap(similarity_matrix, cmap=cmap, norm=norm, cbar=False, square=square, linewidths=0, ax=ax)\n",
    "        else:\n",
    "            sns.heatmap(similarity_matrix, cmap=cmap, cbar=False, square=square, linewidths=0, ax=ax)\n",
    "    else:\n",
    "        cmap = mcolors.ListedColormap([\"black\", \"darkblue\", \"deepskyblue\"])\n",
    "        bounds = [0, low_similarity, high_similarity, 1]\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "        sns.heatmap(similarity_matrix, cmap=cmap, norm=norm, cbar=False, square=square, linewidths=0, ax=ax)\n",
    "\n",
    "    if matched_pairs:\n",
    "        x_coords, y_coords = zip(*matched_pairs)\n",
    "        ax.plot(x_coords, y_coords, marker='o', linestyle='-', color='red', markersize=4, linewidth=1.5)\n",
    "\n",
    "    ax.tick_params(left=False, bottom=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def display_similarity_matrix(similarity_matrix, inverse=False, figsize=(10, 10)):\n",
    "    if inverse:\n",
    "        plot_similarity_heatmap(similarity_matrix.T, figsize=figsize)\n",
    "    else:\n",
    "        plot_similarity_heatmap(similarity_matrix, figsize=figsize)\n",
    "\n",
    "\n",
    "def text_from_row(row_n):\n",
    "    return matched_df.iloc[row_n]['text_fr'], matched_df.iloc[row_n]['text_en']\n",
    "\n",
    "\n",
    "def text_from_pub_number(pub_number):\n",
    "    return matched_df[matched_df.pub_number == pub_number].text_fr.values[0], matched_df[matched_df.pub_number == pub_number].text_en.values[0]\n",
    "\n",
    "\n",
    "\n",
    "# TODO: ALTERNATE VERSION OF ALIGN SENTENCE FOR EDA\n",
    "def align_sentences(sim_matrix, threshold=0.7):\n",
    "    n, m = sim_matrix.shape\n",
    "\n",
    "    weights = np.where(sim_matrix >= threshold, sim_matrix, 0.0)\n",
    "\n",
    "    dp = np.zeros((n+1, m+1), dtype=np.float32)\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            score_match = dp[i-1, j-1] + weights[i-1, j-1]\n",
    "            score_skip_fr = dp[i-1, j]\n",
    "            score_skip_en = dp[i, j-1]\n",
    "\n",
    "            dp[i, j] = max(score_match, score_skip_fr, score_skip_en)\n",
    "\n",
    "    aligned_pairs = []\n",
    "    i, j = n, m\n",
    "    while i > 0 and j > 0:\n",
    "        current_val = dp[i, j]\n",
    "        if np.isclose(current_val, dp[i-1, j]):\n",
    "            i -= 1\n",
    "        elif np.isclose(current_val, dp[i, j-1]):\n",
    "            j -= 1\n",
    "        else:\n",
    "            if weights[i-1, j-1] > 0:\n",
    "                aligned_pairs.append((i-1, j-1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "    aligned_pairs.reverse()\n",
    "    return dp, dp[n, m], aligned_pairs\n",
    "\n",
    "\n",
    "# TODO: ALTERNATE VERSION OF PROCESS ALL ROWS FOR EDA\n",
    "def process_all_rows_debug(fr_eng_correlation_df, parsed_docs_folder, clf):\n",
    "    matched_data = []\n",
    "\n",
    "    for _, row in fr_eng_correlation_df.iterrows():\n",
    "        pub_number = row['pub_number']\n",
    "        filename_fr, filename_en = row['filename_fr'], row['filename_en']\n",
    "\n",
    "        if filename_fr == \"WITHDRAWN\" and filename_en == \"WITHDRAWN\":\n",
    "            continue\n",
    "\n",
    "        fr_link = get_json_file_link(parsed_docs_folder, filename_fr)\n",
    "        if fr_link == None:\n",
    "            continue\n",
    "\n",
    "        if filename_fr == filename_en:\n",
    "            text_fr, text_en = extract_both_languages_from_single_file(fr_link, clf, True)\n",
    "        else:\n",
    "            en_link = get_json_file_link(parsed_docs_folder, filename_en) \n",
    "            if en_link == None:\n",
    "                continue\n",
    "            text_fr, text_en = extract_both_languages_from_two_files(fr_link, en_link, clf, True)\n",
    "\n",
    "        matched_data.append({'pub_number': pub_number, 'text_fr': text_fr, 'text_en': text_en})\n",
    "\n",
    "    return pd.DataFrame(matched_data)"
   ],
   "id": "2677bfe14b892d1e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:58:57.365635Z",
     "start_time": "2025-02-26T12:52:37.278504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "language_classifier = LanguageClassifier()\n",
    "\n",
    "matched_df = process_all_rows_debug(fr_eng_correlation_df, parsed_docs_folder, language_classifier)\n",
    "matched_df['len_fr'] = matched_df['text_fr'].str.len()\n",
    "matched_df['len_en'] = matched_df['text_en'].str.len()\n",
    "matched_df['len_diff'] = np.where(\n",
    "    matched_df[['len_fr', 'len_en']].min(axis=1) == 0,\n",
    "    999,\n",
    "    matched_df[['len_fr', 'len_en']].max(axis=1) / matched_df[['len_fr', 'len_en']].min(axis=1)\n",
    ")\n",
    "matched_df['len_min'] = matched_df[['len_fr', 'len_en']].min(axis=1)\n",
    "matched_df = pd.merge(matched_df, fr_eng_correlation_df, on='pub_number')\n",
    "\n",
    "# save for EDA\n",
    "matched_df.to_pickle('eda_df.pickle')"
   ],
   "id": "ff45159292e9aa78",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:58:57.400159Z",
     "start_time": "2025-02-26T12:58:57.396533Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c7c024b7ceb4b8bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:58:57.454769Z",
     "start_time": "2025-02-26T12:58:57.450762Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "13dcbd4f29aaf8b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:58:57.492142Z",
     "start_time": "2025-02-26T12:58:57.487233Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "538eed2da1ee35ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:58:57.509901Z",
     "start_time": "2025-02-26T12:58:57.507648Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dc475e39332b43c1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
